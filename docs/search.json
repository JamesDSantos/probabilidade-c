[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probabilidade C",
    "section": "",
    "text": "Prefácio",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introdução",
    "section": "",
    "text": "1.1 Vetores aleatórios\nNeste curso vamos utilizar o negrito para designar um vetor. Por exemplo,\n\\[\\textbf{x}=(x_1,…,x_n)\\], é um vetor de comprimento \\(n\\).\nAgora, sejam \\(X_1,…,X_n\\) variáveis aleatórias. Então, dizemos que\n\\[\\textbf{X}=(X_1,…,X_n)\\] é um vetor aleatório (também é usual o termo variável aleatória \\(n\\) -dimensional).\nUtilizamos a vírgula para denotar a interseção de eventos relacionados aos vetores aleatórios. Por exemplo,\n\\[P(X_1\\in A,X_2\\in B)=P(\\{X_1\\in A\\}\\cap\\{X_2\\in B\\})\\],\nAssim como as variáveis aleatórias, os vetores aleatórios possuem funções de densidade/probabilidade e funções de distribuição. Para frisar que mais de uma variável está sendo considerada, utilizamos o adjetivo conjunta. Deste modo, existem para os vetores aleatórias a função de distribuição conjunta e a função de densidade/probabilidade conjunta. Por sua vez, a distribuição de um subvetor de \\(\\textbf{X}\\) é denominada marginal.\nTambém é possível encontrar funções de distribuição ou densidade/probabilidade para o vetor aleatório \\(\\textbf{X}\\) condicionado com o vetor aleatório \\(\\textbf{Y}\\).\nUma vez que essas funções, já conhecidas para o caso univariado, possuem seu equivalente para vetores, é natural expandir conceitos como espança e variância para vetores, assim como a esperança e variância para distribuições condicionais. Nesse momento, novas medidas que medem o relacionamento entre variáveis aleatórias, como covariância e correlação, serão apresentadas.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#vetores-aleatórios",
    "href": "intro.html#vetores-aleatórios",
    "title": "1  Introdução",
    "section": "",
    "text": "Dois dados de seis faces são lançados. Sejam \\(X_1\\) e \\(X_2\\) os resultados do dado 1 e 2, respectivamente. Então, \\(\\textbf{X}=(X_1,X_2)\\) é um vetor aleatório.\n\n\n\n\nDois dados de seis faces são lançados. Sejam \\(X_1\\) e \\(X_2\\) os resultados do dado 1 e 2, respectivamente. Então, \\(P(X_1=3,X_2=5)\\) é o mesmo que \\(P(\\{X_1=3\\}\\cap\\{X_2=5\\})\\). Em palavras, este número representa a probabilidade de sair 3 no primeiro dado e 5 no segundo.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#teoremas-limites",
    "href": "intro.html#teoremas-limites",
    "title": "1  Introdução",
    "section": "1.2 Teoremas limites",
    "text": "1.2 Teoremas limites",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#cadeias-de-markov",
    "href": "intro.html#cadeias-de-markov",
    "title": "1  Introdução",
    "section": "1.3 Cadeias de Markov",
    "text": "1.3 Cadeias de Markov",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "vetores_aleatorios_discretos.html",
    "href": "vetores_aleatorios_discretos.html",
    "title": "2  Distribuição de vetores aleatórios discretos",
    "section": "",
    "text": "2.1 Função de probabilidade conjunta\nOs objetivos deste capítulo são:\nObserve que a função de probabilidade conjunta é a interseção dos eventos \\(\\{X_1=x_1\\},\\ldots,\\{X_n=x_n\\}\\). Portanto, se \\(X_1,\\ldots,X_n\\) são mutuamente independentes, teremos\n\\[P(\\textbf{X}=\\textbf{x})=\\prod_{i=1}^n P(X_i=x_i).\\] Também é possível que subconjuntos de variáveis do vetor sejam independentes. Por exemplo, \\(\\tilde{\\textbf{X}}_1=\\{X_1,\\ldots,X_m\\}\\) podem ser independentes de \\(\\tilde{\\textbf{X}}_2=\\{X_{m+1},\\ldots,X_n\\}\\), o que resulta em\n\\[P(\\textbf{X}=\\textbf{x})=P(\\tilde{\\textbf{X}}_1=\\tilde{\\textbf{x}}_1)P(\\tilde{\\textbf{X}}_2=\\tilde{\\textbf{x}}_2).\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Distribuição de vetores aleatórios discretos</span>"
    ]
  },
  {
    "objectID": "vetores_aleatorios_discretos.html#função-de-probabilidade-conjunta",
    "href": "vetores_aleatorios_discretos.html#função-de-probabilidade-conjunta",
    "title": "2  Distribuição de vetores aleatórios discretos",
    "section": "",
    "text": "Definition 2.1 Sejam \\(X_1,\\ldots,X_n\\) variáveis aleatórias discretas. Então, a função\n\\[P(\\textbf{X}=\\textbf{x})=P(X_1=x_1,\\ldots,X_n=x_n),\\] é denominada função de probabilidade conjunta.\n\n\nProposition 2.1 Seja \\(\\textbf{X}\\) um vetor de variáveis aleatórias discretas. Teremos que \\[P(\\textbf{X}=\\textbf{x})=P(X_1=x_1,\\ldots,X_n=x_n)\\] é uma função de probabilidade se:\n\\[\n\\sum_{\\textbf{x}\\in\\mathbb{Z}^n}P(\\textbf{X}=\\textbf{x})=1\\] e se \\[P(\\textbf{X}=\\textbf{x})\\geq 0,\\;\\;\\forall\\;\\textbf{x}\\in\\mathbb{Z}^n.\\]\n\n\nExample 2.1 Seja \\(\\textbf{X}=(X_1,X_2)\\) um vetor aleatório discreto e considere a função\n\\[P(X_1=x_1,X_2=x_2)=\\left\\{\\begin{array}{ll}\\frac{1}{20},& x_1=1,x_2=1\\\\\n\\frac{4}{20},&x_1=1,x_2=2\\\\\n\\frac{5}{20},&x_1=2,x_2=1\\\\\n\\frac{7}{20},&x_1=2,x_2=2\\\\\n\\frac{2}{20},&x_1=3,x_2=1\\\\\n\\frac{1}{20},&x_1=3,x_2=2\\\\ 0,&\\hbox{caso contrário}\\end{array}\\right.\\]\nObserve que todas as probabilidades são não negativas a soma para todos os pares \\((x_1,x_2)\\in\\mathbb{Z}^2\\) é igual a 1, logo a função dada é de fato uma função de probabilidade conjunta para o vetor \\(\\textbf{X}\\).\n\n\nExercise 2.1 Seja\n\\[P(\\textbf{X}=\\textbf{x})=P(X_1=x_1,X_2=x_2)=cq^{x_1+x_2},\\] onde \\((x_1,x_2)\\in\\{0,1\\}^2\\) e \\(q&gt;0\\). Encontre o valor de \\(c\\) para que \\(P(\\textbf{X}=\\textbf{x})\\) seja uma função de probabilidade.\n\n\n\n\n\nExample 2.2 Considere uma moeda com os números 0 e 1 em cada lado. Considere ainda que os dois resultados são equiprováveis. A moeda é lançada duas vezes. Seja \\(X_i\\) o resultado do \\(i\\)-ésimo lançamento. Considerando que \\(X_1\\) e \\(X_2\\) são independentes, encontre a função de probabilidade conjunta de \\(\\textbf{X}=(X_1,X_2)\\).\nSolução. Para \\(i=1,2\\), teremos que\n\\[P(X_i=0)=P(X_i=1)=\\frac{1}{2}.\\]\nComo \\(X_1\\) e \\(X_2\\) são lançamentos independentes, teremos\n\\[P(\\textbf{X}=(0,0))=P(X_1=0,X_2=0)=P(X_1=0)P(X_2=0)=\\frac{1}{4}.\\] Todos os resultados possíveis são \\(\\{(0,0),(0,1),(1,0),(1,1)\\}\\) e podemos mostrar, de modo análogo ao que foi exposto acima, todos esses eventos têm probabilidade 1/4.\n\n\nExercise 2.2 Lança-se um dado de 6 faces. Seja \\(\\textbf{X}=(X_1,X_2)\\) onde\n\\[X_1=\\left\\{\\begin{array}{ll}1,& \\hbox{ se o resultado é par}\\\\0,&\\hbox{ caso contrário}\\end{array}\\right.\\] e\n\\[X_2=\\left\\{\\begin{array}{ll}1,& \\hbox{ se o resultado é maior que 3}\\\\ 0,&\\hbox{ caso contrário}\\end{array}\\right.\\]\nEncontre a função de probabilidade conjunta de \\(\\textbf{X}\\).\n\n\nExercise 2.3 Considere o vetor aleatório \\(\\mathbf{X} = (X_1, X_2, X_3)\\) e os subconjuntos \\(\\tilde{\\mathbf{X}}_1 = \\{X_1\\}\\) e \\(\\tilde{\\mathbf{X}}_2 = \\{X_2, X_3\\}\\). A função de probabilidade conjunta de \\(\\tilde{\\mathbf{X}}_2\\) é dada pela tabela abaixo:\n\n\n\n\\(x_2 \\setminus x_3\\)\n0\n1\n\\(P(X_2 = x_2)\\)\n\n\n\n\n0\n0,2\n0,3\n0,5\n\n\n1\n0,1\n0,4\n0,5\n\n\n\\(P(X_3 = x_3)\\)\n0,3\n0,7\n1,0\n\n\n\nSabendo que \\(X_1\\) assume valores no conjunto \\(\\{0, 1\\}\\) com probabilidades \\(P(X_1=0)=0,6\\) e \\(P(X_1=1)=0,4\\), e que o bloco \\(\\tilde{\\mathbf{X}}_1\\) é independente do bloco \\(\\tilde{\\mathbf{X}}_2\\):\nCalcule a probabilidade conjunta do vetor completo para o ponto \\((X_1=0, X_2=1, X_3=0)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Distribuição de vetores aleatórios discretos</span>"
    ]
  },
  {
    "objectID": "vetores_aleatorios_discretos.html#função-de-distribuição-conjunta",
    "href": "vetores_aleatorios_discretos.html#função-de-distribuição-conjunta",
    "title": "2  Distribuição de vetores aleatórios discretos",
    "section": "2.2 Função de distribuição conjunta",
    "text": "2.2 Função de distribuição conjunta\n\nDefinition 2.2 A função de distribuição conjunta é definida por\n\\[F(\\textbf{x})=P(X_1\\leq x_1,\\ldots,X_n\\leq x_n)=\\sum_{u_1=-\\infty}^{x_1}\\cdots\\sum_{u_n=-\\infty}^{x_n}P(X_1=u_1,\\ldots,X_n=u_n).\\]\n\n\nExample 2.3 Considere novamente a função de probabilidade conjunta dada por\n\\[P(X=x,Y=y)=\\left\\{\\begin{array}{ll}\\frac{1}{10},& x=1,y=1\\\\\n\\frac{4}{10},&x=1,y=2\\\\\n\\frac{3}{10},&x=2,y=1\\\\\n\\frac{2}{10},&x=2,y=2\\\\\n\\\\ 0,&\\hbox{caso contrário}\\end{array}\\right.\\]\nA função de distribuição conjunta é dada por\n\\[F(x,y)=\\left\\{\\begin{array}{ll}0,& x&lt;1,y&lt;1\\\\\nP(X=1,Y=1)=\\frac{1}{10},& x=1,y=1\\\\\nP(X=1,Y=1)+P(X=1,Y=2)=\\frac{5}{10},&x=1,y=2\\\\\nP(X=1,Y=1)+P(X=2,Y=1)=\\frac{4}{10},&x=2,y=1\\\\\n1,&x\\geq 2,y\\geq 2\\end{array}\\right.\\]\n\n\nExercise 2.4 Considere o vetor \\((X,Y)\\) de variáveis aleatórias com função de probabilidade conjunta dada por\n\\[P(X=x,Y=y)=\\frac{2^{x+y}}{9},\\] com \\((x,y)\\in\\{0,1\\}^2\\). Encontre a função de distribuição conjunta deste vetor aleatório.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Distribuição de vetores aleatórios discretos</span>"
    ]
  },
  {
    "objectID": "vetores_aleatorios_discretos.html#distribuição-marginal",
    "href": "vetores_aleatorios_discretos.html#distribuição-marginal",
    "title": "2  Distribuição de vetores aleatórios discretos",
    "section": "2.3 Distribuição marginal",
    "text": "2.3 Distribuição marginal\n\nDefinition 2.3 Seja \\(\\textbf{X} = (X_1, \\ldots, X_n)\\) um vetor aleatório discreto. Considere um subvetor \\(\\textbf{X}_a\\) com índices \\(a \\subset \\{1, \\ldots, n\\}\\). A função de probabilidade marginal de \\(\\textbf{X}_a\\) é obtida somando-se a função conjunta sobre todos os valores possíveis das variáveis cujos índices não estão em \\(a\\) (denotados por \\(a^c\\)):\\[P(\\textbf{X}_a = \\textbf{x}_a) = \\sum_{\\textbf{x}_{a^c}} P(X_1=x_1, \\ldots, X_n=x_n)\\]onde o somatório é estendido a todos os valores possíveis de cada \\(x_j\\) tal que \\(j \\notin a\\).\n\nNota: No caso de um vetor bidimensional \\(\\textbf{X} = (X, Y)\\), se quisermos a marginal de \\(X\\), marginalizamos \\(Y\\) somando sobre todos os seus valores:\\[P(X=x) = \\sum_{y=-\\infty}^\\infty P(X=x, Y=y)\\]\n\nExample 2.4 Considere a tabela abaixo, cujo corpo contém a função distribuição de probabilidade conjunta das variáveis \\(X\\) e \\(Y\\).\n\\[\\begin{array}{c|cccc}\\hline\n&y\\\\\nx&  1&  2&  3&  4\\\\ \\hline\n1&  0,1&    0,05&   0,02&   0,07\\\\\n2&  0,08&   0,05&   0,1&    0,19\\\\\n3&  0,1&    0,2&    0,04&   0\\\\ \\hline\\end{array}\\]\nVamos encontrar as distribuições marginais de \\(X\\) e \\(Y\\):\n\\[\\begin{align}\nP(X=1)&=\\sum_{y=1}^4P(X=1,Y=y)=0,24\\\\\nP(X=2)&=\\sum_{y=1}^4P(X=2,Y=y)=0,42\\\\\nP(X=3)&=\\sum_{y=1}^4P(X=3,Y=y)=0,34\\end{align}\\] e \\[\\begin{align}\nP(Y=1)&=\\sum_{x=1}^3P(X=x,Y=1)=0,28\\\\\nP(Y=2)&=\\sum_{x=1}^3P(X=x,Y=1)=0,3\\\\\nP(Y=3)&=\\sum_{x=1}^3P(X=x,Y=1)=0,16\\\\\nP(Y=4)&=\\sum_{x=1}^3P(X=x,Y=1)=0,26.\n\\end{align}\\]\nObserve que a função de probabilidade marginal de \\(X\\) é obtida a partir da soma das linhas da tabela, enquanto que a função de probabilidade marginal de \\(Y\\) é obtida a partir da soma das colunas. Nessas funções podem ser colocadas nas margens da tabela:\n\\[\\begin{array}{c|cccc|c}\\hline\n&y\\\\\nx&  1&  2&  3&  4& P(X=x)\\\\ \\hline\n1&  0,10&   0,05&   0,02&   0,07&0,24\\\\\n2&  0,08&   0,05&   0,10&   0,19&0,42\\\\\n3&  0,10&   0,20&   0,04&   0,00&0,34\\\\ \\hline\nP(Y=y)&0,28&0,30&0,16&0,26&\\end{array}\\]\n\n\nExercise 2.5 Considere a tabela de probabilidade conjunta das variáveis aleatórias discretas \\(X\\) (linhas) e \\(Y\\) (colunas) apresentada abaixo, onde um dos valores foi substituído pela constante \\(k\\).\n\\[\\begin{array}{c|ccc}\\hline\n&y\\\\\nx&  1&  2&  3\\\\ \\hline\n1&  0,15&   0,10&   k\\\\\n2&  0,05&   0,20&   0,15\\\\\n3&  0,05&   0,10&   0,10\\\\ \\hline\\end{array}\\]\n\nEncontre o valor de \\(k\\) para que a tabela represente uma distribuição de probabilidade conjunta válida.\nDetermine as funções de probabilidade marginais de \\(X\\) e \\(Y\\).\n\n\n\nExercise 2.6 Sejam \\(X\\) e \\(Y\\) variáveis aleatórias discretas com a seguinte função de probabilidade conjunta:\n\\[P(X=x,Y=y)=\\frac{e^{−x}x^y}{2^xy!},\\] onde \\(x=1,2,\\ldots\\) e \\(y=0,1,\\ldots\\). Encontre a função de probabilidade marginal de \\(X\\).\n\n\nExercise 2.7 Considere novamente o vetor aleatório \\(\\mathbf{X} = (X_1, X_2, X_3)\\) e os subconjuntos \\(\\tilde{\\mathbf{X}}_1 = \\{X_1\\}\\) e \\(\\tilde{\\mathbf{X}}_2 = \\{X_2, X_3\\}\\). A função de probabilidade conjunta de \\(\\tilde{\\mathbf{X}}_2\\) é dada pela tabela abaixo:\n\n\n\n\\(x_2 \\setminus x_3\\)\n0\n1\n\\(P(X_2 = x_2)\\)\n\n\n\n\n0\n0,2\n0,3\n0,5\n\n\n1\n0,1\n0,4\n0,5\n\n\n\\(P(X_3 = x_3)\\)\n0,3\n0,7\n1,0\n\n\n\nSabendo que \\(X_1\\) assume valores no conjunto \\(\\{0, 1\\}\\) com probabilidades \\(P(X_1=0)=0,6\\) e \\(P(X_1=1)=0,4\\), e que o bloco \\(\\tilde{\\mathbf{X}}_1\\) é independente do bloco \\(\\tilde{\\mathbf{X}}_2\\):\n\nMostre que \\(X_1\\) é independente de \\(X_2\\) e que \\(X_1\\) é independente de \\(X_3\\)\nMostre que \\(X_2\\) e \\(X_3\\) não são independentes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Distribuição de vetores aleatórios discretos</span>"
    ]
  },
  {
    "objectID": "vetores_aleatorios_discretos.html#distribuição-condicional",
    "href": "vetores_aleatorios_discretos.html#distribuição-condicional",
    "title": "2  Distribuição de vetores aleatórios discretos",
    "section": "2.4 Distribuição condicional",
    "text": "2.4 Distribuição condicional\n\nDefinition 2.4 Sejam \\(\\textbf{X}\\) e \\(\\textbf{Y}\\) vetores aleatórios discretos. Então, a distribuição de probabilidade condicional de \\(\\textbf{X}\\) dado \\(\\textbf{Y=y}\\) é definida por\n\\[P(\\textbf{X}=\\textbf{x}|\\textbf{Y}=\\textbf{y})=\\frac{P(\\textbf{X}=\\textbf{x},\\textbf{Y}=\\textbf{y})}{P(\\textbf{Y}=\\textbf{y})},\\] e a respectiva função distribuição é dada por\n\\[F(\\textbf{x}|\\textbf{y})=P(\\textbf{X}\\leq \\textbf{x}|\\textbf{Y}=\\textbf{y}).\\]\n\n\nExample 2.5 Considere a seguinte distribuição conjunta:\n\\[P(X=x,Y=y)=\\left\\{\\begin{array}{ll}0,1,&x=0,y=0\\\\\n0,2,&x=0,y=1\\\\\n0,3,&x=1,y=0\\\\\n0,4,&x=1,y=1\\\\\n0,&\\hbox{caso contrário}\\end{array}\\right.\\]\nQual é a distribuição de \\(Y\\) dado \\(X=1\\)?\nSolução: Primeiro, temos que \\[P(X=1)=P(X=1,Y=0)+P(X=1,Y=1)=0,7\\] logo, \\[P(Y=y|X=1)=\\left\\{\\begin{array}{ll}\n\\frac{3}{7},&y=0\\\\\n\\frac{4}{7},&y=1\\\\\n0,&\\hbox{caso contrário}\\end{array}\\right.\\]\n\n\nExercise 2.8 Considere a variável aleatória bidimensional \\((X, Y)\\) com a seguinte função de probabilidade conjunta:\n\\[P(X=x, Y=y) = \\begin{cases}\nk, & x=1, y=1 \\\\\n2k, & x=1, y=2 \\\\\n3k, & x=2, y=1 \\\\\n4k, & x=2, y=2 \\\\\n0, & \\text{caso contrário}\n\\end{cases}\\]\n\nDetermine o valor da constante \\(k\\).\nCalcule a probabilidade marginal de \\(X\\), ou seja, \\(P(X=x)\\).\nEncontre a distribuição de probabilidade condicional de \\(X\\) dado que \\(Y=2\\).\n\n\n\nExercise 2.9 Considere a função de probabilidade abaixo: \\[P(X=x,Y=y)={y \\choose x}\\frac{1}{2^{2y}}\\] onde \\(x=0,\\ldots,y\\) e \\(y=1,2,\\ldots\\). Encontre a função de probabilidade de \\(X\\) dado \\(Y=y\\).\n\n\nExercise 2.10 Sejam\n\\[P(X=x|Y=y)={y\\choose x}\\frac{1}{2^y}\\] onde \\(x=0,\\ldots,y\\) e \\[P(Y=y)=\\frac{e^{−1}}{y!},\\] onde \\(y=0,1,\\ldots\\). Encontre a função de probabilidade de \\(Y\\) dado \\(X=x\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Distribuição de vetores aleatórios discretos</span>"
    ]
  },
  {
    "objectID": "vetores_aleatorios_discretos.html#funções-de-vetores-aleatórios-discretos",
    "href": "vetores_aleatorios_discretos.html#funções-de-vetores-aleatórios-discretos",
    "title": "2  Distribuição de vetores aleatórios discretos",
    "section": "2.5 Funções de vetores aleatórios discretos",
    "text": "2.5 Funções de vetores aleatórios discretos\nSeja \\(X\\) uma variável aleatória e seja \\(g(.)\\) uma função real. Considere a variável \\(Y=g(X)\\), com \\(g:\\mathbb{R}\\rightarrow \\mathbb{Z}\\). É sempre verdade que\n\\[P(Y\\in A)=P(g(X)\\in A)\\].\nSe \\(X\\) é uma variável discreta, então\n\\[P(Y\\in A)=\\sum_{x:g(x)\\in A}P(X=x).\\]\n\nExample 2.6 Seja \\(X\\) uma variável discreta com\n\\[P(X=x)=\\begin{cases}0,1,& x=-1\\\\ 0,2,& x=0,\\\\ 0,3,&x=1\\\\ 0,4,& x=2 \\end{cases}.\\] Vamos encontrar a função de probabilidade de \\(Y=X^2\\).\n\\[\\begin{array}{c|cccc}\\hline\nx&  -1 &    0&  1&  2& \\\\ \\hline\ny=x^2   &1  &0& 1&  4\\\\ \\hline\nP(X=x)  &0,1    &0,2    &0,3&   0,4\\\\\n\\hline \\end{array}\\]\nDeste modo,\n\\[\\begin{align}\nP(Y=0)&=P(X=0)=0,2\\\\\nP(Y=1)&=P(X=-1)+P(X=1)=0,4\\\\\nP(Y=4)&=P(X=2)=0,4\n\\end{align}\\]\n\n\nExercise 2.11 Seja \\(X\\) uma variável discreta com\\[P(X=x)=\\begin{cases}0,2,& x=-1\\\\ 0,1,& x=0,\\\\ 0,4,&x=1\\\\ 0,3,& x=2 \\end{cases}.\\] Encontre a função de probabilidade de \\(Y=|X|\\).\n\nObserve que a extensão é natural para vetores aleatórios. Seja \\(\\textbf{X}\\) um vetor aleatório discreto de comprimento \\(n\\) e seja \\(g:\\mathbb{R}^n\\rightarrow \\mathbb{Z}^m\\). Então a função de probabilidade de \\(\\textbf{Y}=g(\\textbf{X})\\) é dada por\n\\[P(\\textbf{Y}=\\textbf{y})=P(g(\\textbf{X})=\\textbf{y})=\\sum_{\\textbf{x}\\in\\mathbb{Z}^n:g(\\textbf{x})=\\textbf{y}}P(\\textbf{X}=\\textbf{x}).\\]\n\nExample 2.7 Seja \\(\\textbf{X}=(X_1,X_2)\\) um vetor aleatório com função de probabilidade conjunta dada por\n\\[P(\\textbf{X}=\\textbf{x})=\\frac{x_1+x_2}{12},\\] onde \\((x_1,x_2)\\in\\{1,2\\}^2\\). Vamos encontrar a função de probabilidade de \\(\\textbf{Y}=g(\\textbf{X})=(x_1,x_1+x_2).\\) Observe que\n\\[\\begin{array}{cc|cc}\nx_1 &x_2    &y_1 & y_2 \\\\ \\hline\n1   &1    & 1 & 2\\\\\n1   &2   & 1 & 3\\\\\n2   &1   & 2 & 3\\\\\n2   &2   & 2 & 4\\\\ \\hline\n\\end{array}\n\\] Então,\n\\[\\begin{align}\nP(\\textbf{Y}=(1,2))=P(\\textbf{X}=(1,1))=\\frac{2}{12}\\\\\nP(\\textbf{Y}=(1,3))=P(\\textbf{X}=(1,2))=\\frac{3}{12}\\\\\nP(\\textbf{Y}=(2,3))=P(\\textbf{X}=(2,1))=\\frac{3}{12}\\\\\nP(\\textbf{Y}=(2,4))=P(\\textbf{X}=(2,2))=\\frac{4}{12}\\\\\n\\end{align}\\]\n\n\nExercise 2.12 Seja \\(\\textbf{X}=(X_1,X_2)\\) um vetor aleatório com função de probabilidade conjunta dada por\\[P(\\textbf{X}=\\textbf{x})=\\frac{x_1+x_2}{12},\\] onde\\((x_1,x_2)\\in\\{1,2\\}^2\\). Encontre a função de probabilidade de\\(\\textbf{Y}=g(\\textbf{X})=(X_1-X_2, X_1+X_2).\\)\n\nAté o momento, utilizamos um vetor aleatório discreto de comprimento \\(n\\) para encontrar a distribuição de \\(\\textbf{Y}=g(\\textbf{X})\\), também de comprimento \\(n\\). Contudo, é comum ter \\(g:\\mathbb{R}^n\\rightarrow \\mathbb{Z}^m\\), onde \\(m&lt;n\\). Para o caso no qual \\(n=2\\) e \\(m=1\\), teremos\n\\[P(Y=y)=P(g(\\textbf{X})=y)=\\sum_{\\textbf{x}\\in\\mathbb{Z}^2:g(\\textbf{x})=y}P(X_1=x_1,X_2=x_2).\\] Sem perda de generalidade, assuma que para um dado \\(y\\), existe uma função \\(h\\) tal que a condição \\(g(x_1, x_2) = y\\) é equivalente a \\(x_1 = h(x_2, y)\\). Então \\[\\begin{align}P(Y=y)&=\\sum_{\\textbf{x}\\in\\mathbb{Z}^2:g(\\textbf{x})=y}P(X_1=h(x_2,y),X_2=x_2)\\\\&=\\sum_{x_2=-\\infty}^\\infty P(X_1=h(x_2,y),X_2=x_2).\\end{align}\\]\n\nProposition 2.2 Soma de variáveis aleatórias Seja \\(\\textbf{X}=(X_1,X_2)\\) um vetor aleatório discreto e considere da variável \\(Y=g(X_1,X_2)=X_1+X_2\\). Observe que é possível escrever \\[x_1=y-x_2=h(y,x_2),\\] logo \\[P(Y=y)=\\sum_{x_2=-\\infty}^\\infty P(X_1=y-x_2,X_2=x_2).\\]\n\n\nExample 2.8 A função de probabilidade de \\(\\textbf{X}=(X_1,X_2)\\) é dada na tabela abaixo:\n\\[\\begin{array}{c|cc}\\hline & x_1 \\\\  \nx_2 & -1 & 1 \\\\ \\hline\n0 & 0,10 & 0,05\\\\\n1 & 0,15 & 0,20\\\\\n2 & 0,25 & 0,25 \\\\ \\hline\\end{array}\\]\nA função de probabilidade de \\(Y=X_1+X_2\\) é dada por \\[\\begin{align}P(Y=y)&=\\sum_{x_2=0}^2 P(X_1=y-x_2,X_2=x_2)\\\\&=P(X_1=y,X_2=0)+P(X_1=y-1,X_2=1)+P(X_1=y-2,X_2=2)\\end{align}\\] Por exemplo, para \\(y=-1\\) teremos\n\\[\\begin{align}P(Y=-1)&=P(X_1=-1,X_2=0)+P(X_1=-2,X_2=1)+P(X_1=-3,X_2=2)\\\\&=P(X_1=-1,X_2=0)=0,1\\end{align}\\]\n\n\nExercise 2.13 Com base no exemplo anterior, determine os valores restantes da função de probabilidade de \\(Y\\).\n\n\n2.5.1 Alguns resultados importantes\n\n2.5.1.1 Funções indicadoras\nVamos discutir alguns resultados importantes sobre funções de vetores discretos. Contudo, é relevante a discussão de alguns resultados relacionados a funções indicadoras.\n\nFunção indicadora.Seja \\(A \\subset \\Omega\\). A função \\(I_A: \\Omega \\rightarrow \\{0,1\\}\\), definida por\\[I_A(x)=\\begin{cases} 1, & \\text{se } x \\in A \\\\ 0, & \\text{se } x \\notin A \\end{cases}\\]é denominada função indicadora do conjunto \\(A\\)\n\n\nIndicadora da interseção \\[I_{A\\cap B}(x)=I_A(x)I_B(x)\\]\n\n\nExample 2.9 Seja \\(A=\\{x\\in \\mathbb{N}:x \\leq 5 \\}\\) e \\(B=\\{x\\in\\mathbb{N}:x\\hbox{ é ímpar}\\}\\). É simples notar que os valores que satisfazem simultaneamente as restrições de \\(A\\) e \\(B\\) são 1, 3 e 5. Vamos chegar a essa conclusão utilizando indicadoras:\n\\[\\begin{array}{c|cccccc|ccc}\\hline\nx & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & \\cdots \\\\ \\hline\nI_A(x)& 1 & 1 & 1& 1 & 1 & 1 & 0 & 0 & \\cdots & \\\\\\hline\nI_B(x)& 0 & 1 & 0& 1 & 0 & 1 & 0 & 1 & \\cdots & \\\\\\hline\nI_{A\\cap B}(x)& 0 & 1 & 0& 1 & 0 & 1 & 0 & 0 & 0 & \\\\\\hline\\end{array}\\]\n\n\nExercise 2.14 Considere o conjunto universo \\(\\Omega = \\{0, 1, 2, 3, 4, 5, 6, 7\\}\\). Definimos dois subconjuntos de \\(\\Omega\\) através das seguintes propriedades:\\(A = \\{x \\in \\Omega : x \\text{ é múltiplo de 3}\\}\\)\\(B = \\{x \\in \\Omega : x^2 - 7x + 10 \\leq 0\\}\\)A) Liste os elementos de \\(A\\) e \\(B\\) e determine seus respectivos vetores indicadores \\(I_A(x)\\) e \\(I_B(x)\\) para todo \\(x \\in \\Omega\\).B) Utilizando a proposição \\(I_{A \\cap B}(x) = I_A(x)I_B(x)\\), complete a tabela abaixo para identificar os elementos da interseção:\n\\[\\begin{array}{c|cccccccc}\n\\hline\nx & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\\\ \\hline\nI_A(x) & & & & & & & & \\\\ \\hline\nI_B(x) & & & & & & & & \\\\ \\hline\nI_A(x)I_B(x) & & & & & & & & \\\\ \\hline\n\\end{array}\\]C) Com base no resultado da última linha da tabela, escreva o conjunto \\(A \\cap B\\) por extensão.\n\nAs funções indicadoras são úteis para definir para quais valores as probabilidades são positivas. Por exemplo, em vez que escrever\n\\[P(X=x)=p^x(1-p)^{1-x}\\] para \\(x\\in\\{0,1\\}\\), podemos escrever\n\\[P(X=x)=p^x(1-p)^{1-x}I_{\\{0,1\\}}(x).\\]\n\n\n2.5.1.2 O Método da Indução Matemática\nA demonstração por indução é uma ferramentada matemática para provar que uma afirmação é verdadeira para todos os números inteiros a partir de um valor inicial. O método divide-se em dois passos fundamentais:\n\nBase da Indução: Verificamos se a afirmação vale para o primeiro caso (geralmente \\(n=1\\)).\nPasso Indutivo: Assumimos que a afirmação vale para um número \\(k\\) (nossa Hipótese de Indução) e provamos que, a partir disso, ela obrigatoriamente vale para \\(k+1\\).\n\n\nExample 2.10 A Soma dos Primeiros Inteiros. Prove que, para todo \\(n \\geq 1\\):\\[1 + 2 + 3 + \\ldots + n = \\frac{n(n+1)}{2}\\]\nDemonstração:\n\nBase: Para \\(n=1\\):O lado esquerdo é \\(1\\). O lado direito é \\(\\frac{1(1+1)}{2} = 1\\). A base está verificada.\nHipótese de Indução (H.I.): Supomos que para um \\(k\\) qualquer a fórmula é válida:\\[S_k = 1 + 2 + \\ldots + k = \\frac{k(k+1)}{2}\\]\nPasso Indutivo: Queremos mostrar que a soma até \\(k+1\\) segue a mesma lógica. Note que a soma até \\(k+1\\) é a soma até \\(k\\) mais o próximo termo:\\[S_{k+1} = \\underbrace{1 + 2 + \\ldots + k}_{S_k} + (k+1)\\]Substituindo pela nossa H.I.:\\[S_{k+1} = \\frac{k(k+1)}{2} + (k+1)\\]Colocando em um denominador comum:\\[S_{k+1} = \\frac{k(k+1) + 2(k+1)}{2} = \\frac{(k+1)(k+2)}{2}\\]Como chegamos na fórmula original com \\(n\\) substituído por \\(k+1\\), a prova está concluída.\n\n\n\nExercise 2.15 Utilize o método da indução matemática para provar que a soma das primeiras \\(n\\) potências de 2 (começando em \\(2^1\\)) é dada por:\\[2^1 + 2^2 + 2^3 + \\ldots + 2^n = 2^{n+1} - 2\\]\n\n\n\n\n2.5.2 Distribuição Binomial como soma de Bernoullis independentes\n\nRecordando Dizemos que \\(X\\sim\\hbox{Bernoulli(p)}\\) se sua função de probabilidade é dada por\n\\[P(X=x)=p^{x}(1-p)^{1-x}I_{\\{0,1\\}}(x).\\]\nDizemos que \\(Y\\sim\\hbox{Binomial}(n,p)\\) se sua função de probabilidade é dada por\n\\[P(Y=y)={n\\choose y}p^{y}(1-p)^{n-y}I_{\\{0,\\ldots,n\\}}(y).\\]\n\n\nResultado chave: Para \\(0&lt;y&lt;n\\) natural, \\[{n-1\\choose y}+{n-1\\choose y-1}={n\\choose y}.\\]\n\nSejam \\(X_1\\) e \\(X_2\\) variáveis aleatórias independentes com distribuição Bernoulli(\\(p\\)). Vamos encontrar a função de probabilidade de \\(Y=X_1+X_2\\). Primeiro, já sabemos que\n\\[P(Y=y)=\\sum_{x_2=0}^1 P(X_1=y-x_2,X_2=x_2),\\] e, como \\(X_1\\) é independente de \\(X_2\\),\n\\[P(Y=y)=\\sum_{x_2=0}^1 P(X_1=y-x_2)P(X_2=x_2).\\] como \\(X_1\\) e \\(X_2\\) tem distribuição Bernoulli(\\(p\\)), teremos\n\\[\\begin{align}P(Y=y)&=\\sum_{x_2=0}^1 \\left[p^{y-x_2}(1-p)^{1-y+x_2} I_{\\{0,1\\}}(y-x_2)\\right]\\left[p^{x_2}(1-p)^{1-x_2}I_{\\{0,1\\}}(x_2)\\right]\\\\&=p^{y}(1-p)^{2-y}\\sum_{x_2=0}^1 I_{\\{0,1\\}}(x_2)I_{\\{0,1\\}}(y-x_2)\\end{align},\\]\nSabemos que \\(y\\in\\{0,1,2\\}\\). Vamos obter o valor de \\(\\sum_{x_2=0}^1 I_{\\{0,1\\}}(x_2)I_{\\{0,1\\}}(y-x_2)\\) para cada \\(y\\).\n\nse \\(y=0\\),\n\n\\[\\begin{array}{l|cc|c}\n\\hline\nx_2 & 0 & 1 &\\hbox{soma}\\\\ \\hline\nI_{\\{0,1\\}}(x_2) & 1& 1\\\\ \\hline\nI_{\\{0,1\\}}(y-x_2)& 1& 0 \\\\ \\hline\nI_{\\{0,1\\}}(x_2)I_{\\{0,1\\}}(y-x_2) & 1& 0 & 1\\\\ \\hline\n\\end{array}\\] logo, \\(\\sum_{x_2=0}^1 I_{\\{0,1\\}}(x_2)I_{\\{0,1\\}}(0-x_2)=1\\)\n\nse \\(y=1\\),\n\n\\[\\begin{array}{l|cc|c}\n\\hline\nx_2 & 0 & 1 &\\hbox{soma}\\\\ \\hline\nI_{\\{0,1\\}}(x_2) & 1& 1\\\\ \\hline\nI_{\\{0,1\\}}(y-x_2)& 1& 1 \\\\ \\hline\nI_{\\{0,1\\}}(x_2)I_{\\{0,1\\}}(y-x_2) & 1& 1 & 2\\\\ \\hline\n\\end{array}\\] logo, \\(\\sum_{x_2=0}^1 I_{\\{0,1\\}}(x_2)I_{\\{0,1\\}}(1-x_2)=2\\)\n\nse \\(y=2\\),\n\n\\[\\begin{array}{l|cc|c}\n\\hline\nx_2 & 0 & 1 & \\hbox{soma}\\\\ \\hline\nI_{\\{0,1\\}}(x_2) & 1& 1\\\\ \\hline\nI_{\\{0,1\\}}(y-x_2)& 0& 1 \\\\ \\hline\nI_{\\{0,1\\}}(x_2)I_{\\{0,1\\}}(y-x_2) & 0& 1 & 1\\\\ \\hline\n\\end{array}\\] logo, \\(\\sum_{x_2=0}^1 I_{\\{0,1\\}}(x_2)I_{\\{0,1\\}}(2-x_2)=1\\)\nAgora, note que\n\\[\\begin{array}{c|ccc}\\hline\ny & 0 & 1 & 2 \\\\ \\hline\n\\sum_{x_2=0}^1 I_{\\{0,1\\}}(x_2)I_{\\{0,1\\}}(y-x_2) & 1 & 2 & 1 \\\\ \\hline\n{2 \\choose y} &  1 & 2 & 1 \\\\ \\hline\n\\end{array}\\]\nlogo,\n\\[P(Y=y)={2\\choose y}p^y(1-p)^{2-y}I_{\\{0,1,2\\}}(y).\\]\n\nExercise 2.16 Sejam \\(X_1,X_2,X_3\\) variáveis aleatórias independentes com distribuição Bernoulli\\((p)\\). Mostre que a distribuição de \\(Y=X_1+X_2+X_3\\) é\n\\[P(Y=y)={3 \\choose y}p^{y}(1-p)^{3-y}I_{\\{0,1,2,3\\}}(y)\\] Dica: Você já sabe que \\(Z=X_1+X_2\\) é \\[P(Z=z)={2\\choose z}p^z (1-p)^{2-z}I_{\\{0,1,2\\}}(z),\\] e, como \\(Y=X_1+X_2+X_3=Z+X_3\\), basta encontrar \\[P(Y=y)=\\sum_{x_3=0}^1 P(Z=y-x_3,X_3=x_3).\\]\n\nObserve que \\(X_1+X_2\\sim\\hbox{Binomial}(2,p)\\) e \\(X_1+X_2+X_3\\sim\\hbox{Binomial}(3,p)\\). De fato, sejam \\(X_1,\\ldots,X_n\\) variáveis aleatórias independentes com distribuição Bernoulli(\\(p\\)). Suponha, por indução, que\n\\[Z=\\sum_{i=1}^{n-1} X_i\\sim\\hbox{Binomial}(n-1,p).\\] Então \\[Y=\\sum_{i=1}^n X_i=X_n+Z\\] e\n\\[\\begin{align}\nP(Y=y)&=\\sum_{x=0}^1 P(X_n=x, Z=y-x)=\\sum_{x=0}^1 P(X_n=x)P(Z=y-x)\\\\\n&=\\sum_{x=0}^1 P(X_n=x)P(Z=y-x)\\\\&=\\sum_{x=0}^1 \\left[p^x(1-p)^{1-x}I_{\\{0,1\\}}(x)\\right]\\left[{n-1\\choose y-x}p^{y-x}(1-p)^{n-1-y+x}I_{\\{0,\\ldots,n-1\\}}(y-x)\\right]\\\\\n&=p^y(1-p)^{n-y}\\sum_{x=0}^1\\left[{n-1\\choose y-x}I_{\\{0,1\\}}(x)I_{\\{0,\\ldots,n-1\\}}(y-x)\\right].\n\\end{align}\\]\nVamos analizar o somatório acima:\n\nse \\(y=0\\), teremos que \\(I_{\\{0,\\ldots,n-1\\}}(0-x)=1\\) somente quando \\(x=0\\). Logo,\n\n\\[\\sum_{x=0}^1\\left[{n-1\\choose y-x}I_{\\{0,1\\}}(x)I_{\\{0,\\ldots,n-1\\}}(0-x)\\right]={n-1\\choose 0}=1={n\\choose 0}\\]\n\nse \\(y=n\\), teremos que \\(I_{\\{0,\\ldots,n-1\\}}(n-x)=1\\) somente quando \\(x=1\\). Logo,\n\n\\[\\sum_{x=0}^1\\left[{n-1\\choose y-x}I_{\\{0,1\\}}(x)I_{\\{0,\\ldots,n-1\\}}(n-x)\\right]={n-1\\choose n-1}=1={n\\choose n}\\] * para \\(y=1,\\ldots,n-1\\), teremos que \\(I_{\\{0,\\ldots,n-1\\}}(y-x)=1\\) para \\(x=0,1\\). Logo,\n\\[\\sum_{x=0}^1\\left[{n-1\\choose y-x}I_{\\{0,1\\}}(x)I_{\\{0,\\ldots,n-1\\}}(y-x)\\right]={n-1\\choose y}+{n-1\\choose y-1}={n\\choose y}\\]\nportanto,\n\\[\\begin{align}\nP(Y=y)&=p^y(1-p)^{n-y}\\sum_{x=0}^1\\left[{n-1\\choose y-x}I_{\\{0,1\\}}(x)I_{\\{0,\\ldots,n-1\\}}(y-x)\\right]\\\\&={n\\choose y}p^y(1-p)^{n-y}I_{\\{0,\\ldots,n\\}}(y).\n\\end{align}\\]\n\nExercise 2.17 Sejam \\(X_1,X_2\\) variáveis aleatórias independentes, onde \\(X_i\\sim\\hbox{Binomial}(m_i,p),\\) com \\(i=1,2\\).\n\nMostre que \\(Y=X_1+X_2\\sim\\hbox{Binomial}(m_1+m_2,p)\\). Sugestão: escreva \\(X_i\\) como soma de Bernoullis independentes.\nE qual a distribuição de \\(Z=X_1+\\cdots+X_n\\), onde \\(X_i\\sim\\hbox{Binomial}(m_i,p)\\) e \\(X_1,\\ldots,X_n\\) são independentes?\n\n\n\n\n2.5.3 Soma de Poissons independentes\n\nRecordando Dizemos que \\(X\\sim\\hbox{Poisson}(\\lambda)\\) se sua função de probabilidade é dada por\n\\[P(X=x)=\\frac{e^{-\\lambda}\\lambda^x}{x!}I_{\\mathbb{N}}(x)\\] e \\(\\lambda&gt;0\\).\nResultado chave (Teorema binomial): Para quaisquer \\(a,b\\), \\[\\sum_{x=0}^n{ n\\choose x}a^x b^{n-x}=(a+b)^n.\\]\n\nSejam \\(X_1,X_2\\) variáveis aleatórias independentes com distribuição \\(\\hbox{Poisson}(\\lambda)\\). Seja \\(Y=X_1+X_2\\). Note que\n\\[\\begin{align}P(Y=y)&=\\sum_{x_2=0}^\\infty P(X_1=y-x_2,X_2=x_2)\n\\\\&=\\sum_{x_2=0}^\\infty \\frac{e^{-\\lambda}\\lambda^{y-x_2}}{(y-x_2)!}I_{\\mathbb{N}}(y-x_2)\\frac{e^{-\\lambda}\\lambda^{x_2}}{x_2!}I_{\\mathbb{N}}(x_2)\\\\\n&=e^{-2\\lambda}\\lambda^{y}\\sum_{x_2=0}^\\infty \\frac{1}{x_2!(y-x_2)!}I_{\\mathbb{N}}(y-x_2)I_\\mathbb{N}(x_2).\n\\end{align}\\] Para que \\(I_\\mathbb{N}(y-x_2)=1\\), é necessário que \\(x_2\\leq y\\). Logo, \\[\\begin{align}P(Y=y)&=e^{-2\\lambda}\\lambda^{y}\\sum_{x_2=0}^{y} \\frac{1}{x_2!(y-x_2)!}\\\\\n&=\\frac{e^{-2\\lambda}\\lambda^{y}}{y!}\\sum_{x_2=0}^{y} \\frac{y!}{x_2!(y-x_2)!}=\\frac{e^{-2\\lambda}\\lambda^{y}}{y!}\\sum_{x_2=0}^y{ y\\choose x_2}.\n\\end{align}\\]\nAgora, observe que\n\\[\\sum_{x_2=0}^y{ y\\choose x_2}=\\sum_{x_2=0}^y{ y\\choose x_2}1^{x_2}1^{y-x_2}=(1+1)^y=2^y,\\] portanto, \\[\\begin{align}P(Y=y)&=\\frac{e^{-2\\lambda}\\lambda^y}{y!}2^y=\\frac{e^{-2\\lambda}(2\\lambda)^y}{y!}.\n\\end{align}\\]\nIsso implica que \\(X_1+X_2\\sim\\hbox{Poisson}(2\\lambda)\\).\n\nExercise 2.18 Sejam \\(X_1\\) e \\(X_2\\) variáveis aleatórias independentes com \\(X_1\\sim\\hbox{Poisson}(\\lambda_1)\\) e \\(x_2\\sim\\hbox{Poisson}(\\lambda_2)\\).\n\nMostre que \\(X_1+X_2\\sim\\hbox{Poisson}(\\lambda_1+\\lambda_2)\\)\nCom base nesse resultado, qual é a distribuição de \\(\\sum_{i=1}^n X_i\\) quando as variáveis são independentes com \\(X_i\\sim\\hbox{Poisson}(\\lambda_i)\\)?\n\n\n\nExercise 2.19 Sejam \\(X_1\\) e \\(X_2\\) variáveis aleatórias independentes com \\(X_1\\sim\\hbox{Poisson}(\\lambda_1)\\) e \\(x_2\\sim\\hbox{Poisson}(\\lambda_2)\\). Encontre\n\\[P(X_1=x|X_1+X_2=n).\\]\n\n\n\n2.5.4 Distribuição binomial negativa como soma de distribuições geométricas independentes\n\nRecordando Dizemos que \\(X\\sim\\hbox{Geométrica}(p)\\) se sua função de probabilidade é dada por \\[P(X=x)=p(1-p)^x I_{\\mathbb{N}}(x)\\] e dizemos que \\(Y\\sim\\hbox{Binomial Negativa}(n,p)\\) se sua função de probabilidade é dada por \\[P(Y=y)={y+n-1\\choose y}p^n(1-p)^y I_{\\mathbb{N}}(y).\\]\n\n\nResultado relevante: para \\(t&gt;0\\),\n\\[\\sum_{j=0}^m{m+t-j\\choose m-j}={m+t+1\\choose m}\\]\n\n\nExercise 2.20 Sejam \\(X_1,X_2\\) variáveis aleatórias independentes com distribuição Geométrica\\((p)\\). Mostre que \\(Y=X_1+X_2\\sim\\hbox{Binomial Negativa}(2,p)\\)\n\n\nExercise 2.21 Sejam \\(X_1,X_2,X_3\\) variáveis aleatórias independentes com distribuição Geométrica\\((p)\\). Mostre que \\(Y=X_1+X_2+X_3\\sim\\hbox{Binomial Negativa}(3,p)\\)\n\n\nExercise 2.22 Sejam \\(X_1,X_2,\\ldots,X_n\\) variáveis aleatórias independentes com distribuição Geométrica\\((p)\\). Mostre que \\(Y=\\sum_{i=1}^nX_i\\sim\\hbox{Binomial Negativa}(n,p)\\). Especificamente, suponha por indução que\n\\[Z=\\sum_{i=1}^{n-1}X_i\\sim\\hbox{Binomial Negativa}(n-1,p).\\] Então, conclua o exercício encontrando a distribuição de \\(Y=Z+X_n\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Distribuição de vetores aleatórios discretos</span>"
    ]
  },
  {
    "objectID": "vetores_aleatorios_continuos.html",
    "href": "vetores_aleatorios_continuos.html",
    "title": "3  Distribuição de vetores aleatórios contínuos",
    "section": "",
    "text": "3.1 Função de densidade conjunta\nOs objetivos deste capítulo são:\nSeja \\(\\textbf{X}=(X_1,…,X_n)\\) um vetor de variáveis aleatórias absolutamente contínuas. Dizemos que a função contínua\n\\[f(\\textbf{x})=f(x_1,…,x_n)\\] é a função de densidade conjunta de \\(\\textbf{X}\\) se\n\\[P(X_1\\in A_1,\\ldots,X_n\\in A_n)=\\int_{A_1}\\cdots\\int_{A_n}f(\\textbf{x})d\\textbf{x}.\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distribuição de vetores aleatórios contínuos</span>"
    ]
  },
  {
    "objectID": "vetores_aleatorios_continuos.html#função-de-densidade-conjunta",
    "href": "vetores_aleatorios_continuos.html#função-de-densidade-conjunta",
    "title": "3  Distribuição de vetores aleatórios contínuos",
    "section": "",
    "text": "Proposition 3.1 Dizemos que \\(f(\\textbf{x})\\) (contínua) é a função de densidade conjunta para um vetor aleatório \\(\\textbf{X}\\) se\n\\[f(\\textbf{x})\\geq 0,\\;\\;\\forall\\;\\textbf{x}\\in\\mathbb{R}^n\\] e \\[\\int_{\\mathbb{R}^n}f(\\textbf{x})d\\textbf{x}=1.\\]\n\n\nNota importante Observe que \\[P(\\textbf{X}\\in A)=\\int_{A}f(\\textbf{x})d\\textbf{x}=\\int_{\\mathbb{R}^n}f(\\textbf{x})I_{A}(\\textbf{x})d\\textbf{x}.\\]\nEm palavras, o conjunto da função indicadora é sempre o conjunto de integração.\n\n\nExample 3.1 Mostre que \\[f(x,y)=xe^{−(1+y)x},\\] com \\(x,y&gt;0\\) é uma função de densidade conjunta.\nSolução. Note que \\(x&gt;0\\), logo \\(f(x,y)&gt;0\\) para todo \\((x,y)&gt;0\\). Portanto, basta mostrar que\n\\[\\int_{\\mathbb{R}^2} f(x,y)dxdy=\\int_{\\mathbb{R}^2}xe^{-(1+y)x}I_{\\mathbb{R}^+}(x)I_{\\mathbb{R}^+}(y)dxdy=\\int_0^\\infty\\int_0^\\infty xe^{-(1+y)x}dxdy=1\\]\nRecorde que, para \\(a,b&gt;0\\), \\[\\int_0^\\infty u^{a-1}e^{-bu}du=\\frac{\\Gamma(a)}{b^a}.\\]\nPortanto, considerando apenas a integral em \\(x\\), teremos\n\\[\\int_0^\\infty xe^{-x(1+y)}dx=\\frac{\\Gamma(2)}{(1+y)^2}=\\frac{1!}{(1+y)^2}=\\frac{1}{(1+y)^2}.\\] Então, \\[\\int_0^\\infty\\int_0^\\infty xe^{-(1+y)x}dxdy=\\int_0^\\infty \\frac{1}{(1+y)^2}dy\\]\nA integral da direita pode ser resolvida por integração por substituição. Fazendo \\(u=1+y\\), teremos e \\(du=dy\\) e\n\\[\\begin{align}\\int_0^\\infty \\frac{1}{(1+y)^2}I_{\\mathbb{R}^+}(y)dy&=\\int_0^\\infty \\frac{1}{u^2}I_{\\mathbb{R}^+ }(u-1)du\\\\&=\\int_1^\\infty \\frac{1}{u^2}du=\\left.-\\frac{1}{u}\\right|_1^\\infty=1.\\end{align}\\] Portanto, a função dada é uma densidade conjunta.\n\n\nExercise 3.1 Mostre que \\[f(x,y)=x^2e^{−(1+y)x},\\] com \\(x,y&gt;0\\) é uma função de densidade conjunta.\n\n\nExample 3.2 Seja \\[f(x,y)=kx^2,\\] onde \\(x\\in(0,1)\\) e \\(y\\in(0,x)\\). Encontre o valor de \\(k\\) para que \\(f(x,y)\\) seja uma função densidade.\nSolução. Como \\(x&gt;0\\), temos que \\(f(x,y)&gt;0\\) sempre que \\(k&gt;0\\). Portanto, basta encontrar o valor \\(k&gt;0\\) tal que\n\\[\\int_{\\mathbb{R}^2}f(x,y)dxdy=\\int_{\\mathbb{R}^2}kx^2 I_{(0,1)}(x)I_{(0,x)}(y)dxdy=\\int_0^1\\int_0^x kx^2 dydx=1\\]\nA integral em \\(y\\) é\n\\[\\int_0^x kx^2 dy=\\left.kx^2y\\right|_0^x=kx^3\\] logo, \\[\\int_0^1\\int_0^x kx^2 dydx=\\int_0^1 kx^3dx=\\left.\\frac{k}{4}x^4\\right|_0^1=\\frac{k}{4}.\\] Portanto, \\(f(x,y)\\) será função densidade conjunta quando \\(k=4\\).\n\n\nExercise 3.2 Seja \\[f(x,y)=k yx^2,\\] onde \\(x\\in(0,1)\\) e \\(y\\in(0,x)\\). Encontre o valor de \\(k\\) para que \\(f(x,y)\\) seja uma função densidade.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distribuição de vetores aleatórios contínuos</span>"
    ]
  },
  {
    "objectID": "vetores_aleatorios_continuos.html#função-de-distribuição-conjunta",
    "href": "vetores_aleatorios_continuos.html#função-de-distribuição-conjunta",
    "title": "3  Distribuição de vetores aleatórios contínuos",
    "section": "3.2 Função de distribuição conjunta",
    "text": "3.2 Função de distribuição conjunta\n\nNota importante Recorde que \\[I_{A\\cap B}(\\textbf{x})=I_{A}(\\textbf{x})I_{B}(\\textbf{x}).\\] Em particular, sejam \\(A\\) e \\(B\\) os intervalos \\((a_1,a_2)\\) e \\((b_1,b_2)\\). Então\n\\[I_{A\\cap B}(x)=I_{(a_1,a_2)}(x)I_{(b_1,b_2)}(x)=I_{(\\max\\{a_1,b_1\\},\\min\\{a_2,b_2\\})}(x).\\]\n\n\nExample 3.3 Simplificação de Indicadoras Simplifique o produto das seguintes funções indicadoras em uma única função indicadora:\\[g(x) = I_{(2, 10)}(x)I_{(5, 15)}(x)I_{(-\\infty, 8)}(x)\\]\nSolução:\nAplicamos a propriedade da interseção passo a passo, utilizando \\(I_{A \\cap B}(x) = I_{(\\max\\{a_1,b_1\\}, \\min\\{a_2,b_2\\})}(x)\\).\n\nPrimeiro par: \\(I_{(2, 10)}(x)I_{(5, 15)}(x)\\)\nLimite inferior: \\(\\max\\{2, 5\\} = 5\\)\nLimite superior: \\(\\min\\{10, 15\\} = 10\\)\nIndicadora resultante: \\(I_{(5,10)}(x)\\)\n\nMultiplicando pelo terceiro termo: \\(I_{(5, 10)}(x)I_{(-\\infty, 8)}(x)\\)\nLimite inferior: \\(\\max\\{5, -\\infty\\} = 5\\) Limite superior: \\(\\min\\{10, 8\\} = 8\\) *Resultado final: \\(I_{(5, 8)}(x)\\)\n\n\nExercise 3.3 Seja a função \\(h(x)\\) definida pelo produto de três restrições de intervalos:\\[h(x) = I_{(-3, 4)}(x)I_{(0, 7)}(x) \\cdot I_{(x, \\infty)}(2)\\]Dica: Note que \\(I_{(x, \\infty)}(2)\\) é o mesmo que dizer que \\(x &lt; 2\\).\n\nReescreva \\(h(x)\\) como uma única função indicadora.\nDetermine o valor da integral \\(\\int_{-\\infty}^{\\infty} h(x) dx\\).\n\n\n\nExercise 3.4 Simplifique a expressão abaixo e determine o valor da função para qualquer \\(x \\in \\mathbb{R}\\):\\[g(x) = I_{(0, 5)}(x)I_{(10, 20)}(x)\\]\n\n\nExercise 3.5 Expresse o produto das indicadoras abaixo como uma única indicadora de \\(x\\), mantendo \\(y\\) como um parâmetro fixo:\\[k(x) = I_{(0, \\infty)}(x) I_{(-\\infty, y)}(x)\\]\n\nDetermine a indicadora resultante.\nPara quais valores de \\(y\\) a função \\(k(x)\\) é identicamente nula (igual a zero para todo \\(x\\))?\n\n\nComo vimos no capítulo anterior, \\[F(\\textbf{x})=P(\\textbf{x}\\leq \\textbf{x})=P(X_1\\leq x_1,\\ldots,X_n\\leq x_n),\\] é denominada função de distribuição conjunta (ou ainda função de distribuição multivariada).\n\nProposition 3.2 A função de distribuição de um vetor de variáveis aleatórias contínuas é dada por \\[F(\\textbf{x})=\\int_{-\\infty}^{x_1}\\cdots\\int_{-\\infty}^{x_n}f(y_1,\\ldots,y_n)d\\textbf{y}.\\]\n\n\nExample 3.4 Encontre a função distribuição do vetor \\((X,Y)\\) cuja função de densidade conjunta é\n\\[f(x,y)=2xI_{(0,1)}(x)I_{(0,1)}(y).\\] Solução.\n\\[\\begin{align}F(u,v)&=\\int_{-\\infty}^u\\int_{-\\infty}^v f(x,y)dydx=\\int_{-\\infty}^u\\int_{-\\infty}^v 2xI_{(0,1)}(x)I_{(0,1)}(y)dydx\\end{align}\\] Considerando que \\(u,v\\in(0,1)\\), teremos\n\\[\\begin{align}F(u,v)&=\\int_{0}^u\\int_{0}^v 2xI_{(0,1)}(x)I_{(0,1)}(y)dydx.\\end{align}\\] A integral em \\(y\\) é \\[\\int_0^{v}2xdy=2x\\left.y\\right|_{0}^{v}=v\\] logo, \\[F(u,v)=2v\\int_0^{u}xdx=2v\\left.\\frac{x^2}{2}\\right|_0^u=vu^2\\] para \\(u,v\\in(0,1)\\).\n\n\nExercise 3.6 Encontre a função de distribuição conjunta a partir da densidade conjunta abaixo:\n\\[f(x,y)=\\frac{3}{80}(x^2+xy)\\], com \\(0&lt;x&lt;2\\) e \\(0&lt;y&lt;4\\).\n\n\nExample 3.5 Considere o vetor aleatório com função densidade conjunta dada por \\[f(x,y)=e^{-y}I_{(0,\\infty)}(x)I_{(x,\\infty)}(y).\\] Encontre a função de distribuição conjunta.\n\\[\\begin{align}F(u,v)&=\\int_{-\\infty}^u\\int_{-\\infty}^v f(x,y)dydx=\\int_{-\\infty}^u\\int_{-\\infty}^v e^{-y}I_{(0,\\infty)}(x)I_{(x,\\infty)}(y)dydx\\\\&=\\int_{-\\infty}^uI_{(0,\\infty)}(x)\\left[\\int_{-\\infty}^v e^{-y}I_{(x,\\infty)}(y)dy\\right]dx\\end{align}\\] Observe que podemos fazer \\[\\int_{-\\infty}^v e^{-y}I_{(x,\\infty)}dy=\\int_\\mathbb{R}e^{-y}I_{(x,\\infty)}(y)I_{\\mathbb{R}^+}(y)dy\\] Note que \\(I_{(-\\infty,v)}(y)I_{(x,\\infty)}(y)=1\\) somente se \\(x&lt;v\\). Isso é o mesmo que escrever\n\\[1=I_{(-\\infty,v)}(y)I_{(x,\\infty)}(y)I_{(-\\infty,v)}(x)=I_{(x,v)}(y)I_{(-\\infty,v)}(x).\\] Então,\n\\[\\begin{align}\\int_{-\\infty}^v e^{-y}I_{(x,\\infty)}dy&=I_{(-\\infty),v}(x)\\int_x^ve^{-y}dy=I_{(-\\infty,v)}\\left[\\left.-e^{-y}\\right|_{x}^v\\right]\\\\&=\\left(e^{-x}-e^{-v}\\right)I_{(-\\infty,v)}(x)\\end{align}\\]\nEntão \\[\\begin{align}F(u,v)&=\\int_{-\\infty}^{u}I_{(0,\\infty)}(x)I_{(-\\infty,v)}(x)\\left(e^{-x}-e^{-v}\\right)dx\\\\&=\\int_{-\\infty}^{u}I_{(0,v)}(x)\\left(e^{-x}-e^{-v}\\right)dx\\\\&=\\int_0^{\\min\\{u,v\\}}(e^{-x}-e^{-v})dx\\\\&=\\int_0^{\\min\\{u,v\\}}e^{-x}dx-\\int_0^{\\min\\{u,v\\}}e^{-v}dx\\\\&=\\left[\\left.-e^{-x}\\right|_0^{\\min\\{u,v\\}}\\right]- \\min\\{u,v\\}e^{-v}\\\\&=1-e^{-\\min\\{u,v\\}}-\\min\\{u,v\\}e^{-v}\\end{align}\\] para \\(u,v&gt;0\\).\n\n\nExercise 3.7 Considere o vetor aleatório \\((X, Y)\\) com função densidade conjunta dada por:\\[f(x, y) = 2e^{-(x+y)} I_{(0, y)}(x) I_{(0, \\infty)}(y)\\]\nEncontre a função de distribuição conjunta.\n\n\nExercise 3.8 Seja a função densidade conjunta:\\[f(x, y) =\\frac{3}{2}I_{(-1,1)}(x)I_{(x^2,1)}(y).\\] Encontre a função de distribuição.\n\n::: ::: {#prp-} Se \\(\\textbf{X}\\) é um vetor aleatório comprimento \\(n\\), então\n\\[f(\\textbf{x})=\\frac{\\partial}{\\partial x_1}\\cdots \\frac{\\partial}{\\partial x_n} F(\\textbf{x}).\\]\n:::\n\nExercise 3.9 Seja\n\\[F(x,y)=1−e^{−x}−e^{−y}+e^{-(x+y)},\\] com \\((x,y)\\in\\mathbb{R}^2_+\\). Encontre a densidade conjunta correspondente.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distribuição de vetores aleatórios contínuos</span>"
    ]
  },
  {
    "objectID": "vetores_aleatorios_continuos.html#distribuições-marginais",
    "href": "vetores_aleatorios_continuos.html#distribuições-marginais",
    "title": "3  Distribuição de vetores aleatórios contínuos",
    "section": "3.3 Distribuições Marginais",
    "text": "3.3 Distribuições Marginais\nSeja \\(\\textbf{X}\\) um vetor aleatório de dimensão \\(n\\). A distribuição de uma coordenada qualquer de \\(\\textbf{X}\\) é denominada distribuição marginal.\nComo em geral utilizamos as letras \\(F\\), \\(P\\) e \\(f\\) para designar as funções de distribuição, probabilidade e densidade, é importante acrescentar uma notação que identifique explicitamente a variável. Quando isto for necessário, adicionaremos a letra que designa a variável subscrita na função. Por exemplo, \\(F_{X_1}(x)\\) é a função de distribuição marginal de \\(X_1\\).\n\nProposition 3.3 Seja \\(\\textbf{X}=(\\textbf{X}_a,\\textbf{X}_b)\\) um vetor de variáveis aleatórias contínuas. A função de densidade marginal do subvetor \\(\\textbf{X}_a\\) é dada por \\[f_{\\textbf{X}_a}(\\textbf{x}_a)=\\int f(\\textbf{x})d\\textbf{x}_{b}.\\]\n\n\nExample 3.6 Seja \\((X,Y)\\) um par de variáveis aleatórias com densidade conjunta dada por \\[f(x,y)=8xyI_{(0,y)}(x)I_{(0,1)}(y),\\] Determine as densidades marginais de \\(X\\) e \\(Y\\).\nSolução A densidade marginal de \\(Y\\) é\n\\[\\begin{align}f_Y(y)&=\\int_{\\mathbb{R}}f(x,y)dx=8yI_{(0,1)}(y)\\int_\\mathbb{R} I_{(0,y)}(x)xdx\\\\&=8yI_{(0,1)}(y)\\int_0^y xdx=8yI_{(0,1)}(y)\\left.\\frac{x^2}{2}\\right|_0^y=4y^3I_{(0,1)}(y).\\end{align}\\]\nPara determinar a marginal de \\(X\\), observe que\n\\[I_{(0,y)}(x)I_{(0,1)}(y)=1\\Rightarrow 0&lt;x&lt;y\\hbox{ e } 0&lt;y&lt;1\\] note que é possível inferir que: \\(0&lt;x&lt;1\\) e \\(x&lt;y&lt;1\\). Portanto,\n\\[I_{(0,y)}(x)I_{(0,1)}(y)=I_{(x,1)}(y)I_{(0,1)}(x),\\] logo \\[f_X(x)=\\int_x^1 f(x,y)dx=8xI_{(0,1)}(x)\\left.\\frac{y^2}{2}\\right|_{x}^1=4x(1-x^2)I_{(0,1)}(x).\\]\n\n\nExercise 3.10 Seja a função densidade conjunta:\\[f(x, y) =\\frac{3}{2}I_{(-1,1)}(x)I_{(x^2,1)}(y).\\] Encontre as densidades marginais de \\(X\\) e \\(Y\\).\n\n\nExercise 3.11 Considere a seguinte função de densidade conjunta, \\[f(x,y)=xy^2e^{−y(x+1)}.\\] onde \\(x,y&gt;0\\). Encontre a função densidade marginal de \\(X\\) e \\(Y\\).\n\n\nExercise 3.12 Considere a densidade conjunta \\[f(x,y)=\\frac{6}{7}\\left(x^2+\\frac{xy}{2}\\right)\\] para \\(0&lt;x&lt;1\\) e \\(0&lt;y&lt;2\\)\n\nverifique que \\(f(x,y)\\) é de fato uma função densidade conjunta .\nencontre a função densidade marginal de \\(X\\).\nCalcule \\(P(X&gt;Y)\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distribuição de vetores aleatórios contínuos</span>"
    ]
  },
  {
    "objectID": "vetores_aleatorios_continuos.html#distribuição-condicional",
    "href": "vetores_aleatorios_continuos.html#distribuição-condicional",
    "title": "3  Distribuição de vetores aleatórios contínuos",
    "section": "3.4 Distribuição condicional",
    "text": "3.4 Distribuição condicional\nSejam \\(\\textbf{X}\\) e \\(\\textbf{Y}\\) vetores aleatórios contínuos. Sabemos que\n\\[P(\\textbf{X}\\leq \\textbf{x}|\\textbf{Y}\\in B)=\\frac{P(\\textbf{X}\\leq\\textbf{x},\\textbf{Y}\\in B)}{P(\\textbf{Y}\\in B)}.\\]\nComo \\(\\textbf{Y}\\) é um vetor de variáveis aleatórias contínuas, sabemos que \\(P(\\textbf{Y}=\\textbf{y})=0\\). Entretanto, a probabilidade \\(P(\\textbf{X}\\leq \\textbf{x}|\\textbf{Y}=\\textbf{y})\\) faz todo o sentido. Para mostrar este fato, seja \\(B(\\varepsilon)\\) uma bola fechada de raio \\(\\varepsilon\\) e centro \\(\\textbf{y}\\). Pelo Teorema do Valor Médio, existe \\(\\tilde{y}\\in B(\\varepsilon)\\) tal que\n\\[P(\\textbf{Y}\\in B(\\varepsilon) )=\\int_{B(\\varepsilon)} f_{\\textbf{Y}}(\\textbf{u})d\\textbf{u}=\\hbox{Vol}(B(\\varepsilon))f_{\\textbf{Y}}(\\tilde{\\textbf{y}})\\] e\n\\[P(\\textbf{X}\\leq \\textbf{x},\\textbf{Y}\\in B(\\varepsilon) )=\\int_{\\textbf{v}\\leq \\textbf{x}}\\int_{B(\\varepsilon)} f_{\\textbf{X},\\textbf{Y}}(\\textbf{v},\\textbf{u})d\\textbf{u}d\\textbf{v}=\\hbox{Vol}(B(\\varepsilon))\\int_{\\textbf{v}\\leq \\textbf{x}}f_{\\textbf{X},\\textbf{Y}}(\\textbf{v},\\tilde{\\textbf{y}}_{\\textbf{v}})d\\textbf{v}.\\] Portanto,\n\\[P(\\textbf{X}\\leq \\textbf{x}|\\textbf{Y}\\in B(\\varepsilon) )=\\frac{\\hbox{Vol}(B(\\varepsilon))\\int_{\\textbf{v}\\leq \\textbf{x}}f_{\\textbf{X},\\textbf{Y}}(\\textbf{v},\\tilde{\\textbf{y}})d\\textbf{v}}{\\hbox{Vol}(B(\\varepsilon))f_{\\textbf{Y}}(\\tilde{\\textbf{y}})}=\\int_{\\textbf{v}\\leq \\textbf{x}}\\frac{f_{\\textbf{X},\\textbf{Y}}(\\textbf{v},\\tilde{\\textbf{y}}_{\\textbf{v}})}{f_{\\textbf{Y}}(\\tilde{\\textbf{y}})}d\\textbf{v}.\\] Agora, note que \\(B(\\varepsilon)\\rightarrow\\textbf{y}\\) quando \\(\\varepsilon\\rightarrow0\\), logo \\(\\tilde{\\textbf{y}}\\rightarrow\\textbf{y}\\) e \\(\\tilde{\\textbf{y}}_{\\textbf{v}}\\rightarrow\\textbf{y}\\) para todo \\(\\textbf{v}\\), o que implica em\n\\[\\lim_{\\varepsilon\\rightarrow 0}P(\\textbf{X}\\leq \\textbf{x}|\\textbf{Y}\\in B(\\varepsilon) )=P(\\textbf{X}\\leq \\textbf{x}|\\textbf{Y}= \\textbf{y})=F(\\textbf{x}|\\textbf{y})=\\int_{\\textbf{v}\\leq \\textbf{x}}\\frac{f_{\\textbf{X},\\textbf{Y}}(\\textbf{v},{\\textbf{y}})}{f_{\\textbf{Y}}({\\textbf{y}})}d\\textbf{v}.\\] o que implica em\n\\[f(\\textbf{x}|\\textbf{y})=\\frac{f_{\\textbf{X},\\textbf{Y}}(\\textbf{x},\\textbf{y})}{f_{\\textbf{Y}}(\\textbf{y})}.\\]\n\nExample 3.7 Seja \\[f(x,y)=21x^2y^3,\\] se \\(0&lt;x&lt;y&lt;1\\). Encontre a densidade condicional de \\(Y|X=x\\).\nSolução.\nPrimeiro, vamos encontrar a densidade marginal de \\(X\\):\n\\[\\begin{align}f_X(x)&=21x^2I_{(0,1)}(x)\\int_\\mathbb{R} y^3I_{(x,1)}(y)dy=21x^2I_{(0,1)}(x)\\int_x^1 y^3dy\\\\&=21\\left[\\left.\\frac{y^4}{4}\\right|_{x}^1\\right]I_{(0,1)}(x)\\\\&=\\frac{21}{4}x^2(1-x^4)I_{(0,1)}(x),\\end{align}\\]\npara \\(x\\in(0,1)\\). Portanto,\n\\[f(y|x)=\\frac{f(x,y)}{f_X(x)}=\\frac{21x^2y^3}{\\frac{21}{4}x^2(1-x^4)}=\\frac{4y^3}{(1-x^4)}I_{(x,1)}(y),\\]\n\n\nExercise 3.13 Suponha que\n\\[f(x,y)=\\frac{1}{2\\pi}\\exp\\left\\{−\\frac{1}{2}(x^2+2y^2−2xy)\\right\\},\\] com \\((x,y)\\in\\mathbb{R}^2\\). Encontre a densidade de \\(X|Y=y\\).\n\n\nExercise 3.14 Para \\(n&gt;0\\) inteiro, suponha que \\(X\\sim\\hbox{Gama}(n+1,x)\\). Além disso, suponha que dado \\(x\\), \\(Y_1,\\ldots,Y_n\\) são variáveis aleatórias independentes que possuem a seguinte densdidade condicional:\n\\[f(y_i|x)=\\frac{1}{x},\\] para \\(0&lt;y_i&lt;x\\).\n\nDetermine a densidade marginal do vetor \\((Y_1,\\ldots,Y_n)\\)\nDetermine a densidade condicional de \\(X\\) dado os valores de \\(Y_1,\\ldots,Y_n\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distribuição de vetores aleatórios contínuos</span>"
    ]
  },
  {
    "objectID": "vetores_aleatorios_continuos.html#função-de-vetores-aleatórios-contínuos",
    "href": "vetores_aleatorios_continuos.html#função-de-vetores-aleatórios-contínuos",
    "title": "3  Distribuição de vetores aleatórios contínuos",
    "section": "3.5 Função de vetores aleatórios contínuos",
    "text": "3.5 Função de vetores aleatórios contínuos\n\n3.5.1 Caso para uma variável\nSuponha que \\(X\\) é uma variável aleatória contínua e \\(g(.)\\) uma função real e contínua. Considerando a variável aleatória \\(Y=g(X)\\), teremos que\n\\[P(Y\\in A)=P(g(X)\\in A)=\\int_{\\{x:g(x)\\in A\\}}f_X(x)dx.\\]\nEm particular,\n\\[F_Y(y)=P(Y\\leq y)=P(g(X)\\leq y)=\\int_{\\{x:g(x)\\leq y\\}}f_X(x)dx.\\]\nAgora, suponha \\(g(.)\\) possui inversa. Então, podemos utilizar o método integral por substituição. Fazendo \\(u=g(x)\\), teremos que \\(x=g^{−1}(u)\\) e \\(dx=\\frac{d}{du}g^{−1}(u)du\\). Além disso,\n\\[\\{x:g(x)\\leq y\\}\\equiv\\{u\\leq y\\},\\] o que implica em\n\\[F_Y(y)=\\int_{-\\infty}^y f_X(g^{−1}(u))\\left|\\frac{d}{du}g^{−1}(u)\\right|du\\] logo, \\[f_Y(y)=f_X(g^{−1}(y))\\left|\\frac{d}{dy}g^{−1}(y)\\right|.\\] O termo dentro do módulo é denominado Jacobiano da transformação e a obtenção da densidade por esse método é denominada método do Jacobiano.\n\nExample 3.8 Suponha que \\(X\\sim\\hbox{Normal}(0,1)\\). Considere a transformação \\(Y=e^{X}\\). Como \\(g^{-1}(y)=\\log(y)\\), teremos que\n\\[f_Y(y)=f_X(\\log y )\\left|\\frac{d}{dy}\\log y\\right|=\\frac{1}{y\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\log y)^2}.\\] Essa distribuição é denominada Lognormal(0,1).\n\n\nExercise 3.15 Seja \\(X\\) uma variável aleatória com distribuição Uniforme no intervalo \\((0, 1)\\), ou seja, sua função de densidade de probabilidade é: \\[f_X(x) = I_{(0,1)}(x).\\] Encontre a função densidade de \\(Y = -\\ln(X)\\).\n\n\n\n3.5.2 Caso vetorial com \\(g\\) invertível\nAssim como no caso discreto, a extensão para vetores também é natural. Seja \\(\\textbf{X}\\) um vetor aleatório de comprimento \\(n\\) e considere a nova variável \\(\\textbf{Y}=g(X)\\), onde \\(g:\\mathbb{R}^n\\rightarrow \\mathbb{R}^m\\) é uma função real. Então\n\\[P(\\textbf{Y}\\in A)=P(g(\\textbf{X})\\in A)=\\int_{\\{\\textbf{x}:g(\\textbf{x})\\in A\\}}f_\\textbf{X}(\\textbf{x})d\\textbf{x}.\\]\nAgora, suponha que \\(g(.)\\) possui inversa (o que implica que \\(g:\\mathbb{R}^n\\rightarrow \\mathbb{R}^n\\)). Então, pelo método da integral por substituição, teremos que \\(\\textbf{u}=g^{-1}(\\textbf{y})\\), \\(d\\textbf{x}=|\\mathcal{J}g^{-1}(\\textbf{u})|d\\textbf{u}\\) e \\(\\{\\textbf{x}:g(\\textbf{x})\\in A\\}=\\{\\textbf{u}\\in A\\}\\), o que implica em\n\\[P(\\textbf{Y}\\in A)=\\int_{A}f_\\textbf{X}(g^{-1}(\\textbf{u})|\\det\\mathcal{J}g^{-1}(\\textbf{u})|d\\textbf{u}.\\]\nonde \\(\\mathcal{J}g^{-1}(\\textbf{u})\\) é denominada matriz Jacobiana, cujo o elemento \\((i,j)\\) é dado por \\[\\mathcal{J}_{i,j}=\\frac{\\partial x_i}{\\partial u_j}.\\]\nEm particular, teremos que\n\\[F_{\\textbf{Y}}(\\textbf{y})=\\int_{-\\infty}^{y_1}\\cdots \\int_{-\\infty}^{y_n}f_\\textbf{X}(g^{-1}(\\textbf{u}))|\\det\\mathcal{J}g^{-1}(\\textbf{u})|d\\textbf{u}.\\]\nPortanto, concluímos que \\[f_\\textbf{Y}(\\textbf{y})=f_\\textbf{X}(g^{−1}(\\textbf{y}))|\\det\\mathcal{J}g^{-1}(\\textbf{y})|.\\]\n\nExample 3.9 Sejam \\(X_1\\) e \\(X_2\\) variáveis aleatórias independentes com distribuição Normal(0,1). Vamos encontrar a função densidade conjunta de \\(\\textbf{Y}=(X_1+X_2,Y2=X_1−X_2)\\).\nPrimeiro, temos que encontrar a função inversa:\n\\[\\begin{array}{c}\nY_1=X_1+X_2\\\\\nY_2=X_1-X_2\\\\\n\\end{array} \\Leftrightarrow \\begin{array}{c}X_1=0,5(Y_1+Y_2)\\\\ X_2=0,5(Y_1-Y_2)\\end{array}\\]\ne o Jacobiano da transformação é dado por\n\\[\\mathcal{J}g^{-1}(\\textbf{y})=\\left[\\begin{array}{cc} 0,5 & 0,5 \\\\ 0,5 & - 0,5 \\end{array}\\right],\\]\ncujo determinante é igual a -1/2. Logo,\n\\[\\begin{align}f_\\textbf{Y}(\\textbf{y})&=f_{X_1}(0,5(y_1+y_2)f_{X_2}(0,5(y_1−y_2)\\frac{1}{2}\\\\&=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}[0,5(y_1+y_2)]^2}I_{\\mathbb{R}}(0,5(y_1+y_2))\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}[0,5(y_1-y_2)]^2}I_{\\mathbb{R}}(0,5(y_1-y_2))\\frac{1}{2}\\\\&=\\frac{1}{4\\pi}e^{-\\frac{1}{8}[(y_1-y_2)^2+(y_1+y_2)^2]}I_{\\mathbb{R}}(y_1)I_{\\mathbb{R}}(y_2)\\\\&=\\frac{1}{4\\pi}e^{-\\frac{1}{4}(y_1^2+y_2^2)}I_{\\mathbb{R}}(y_1)I_{\\mathbb{R}}(y_2).\\end{align}\\] Podemos notar que a densidade conjunta fatora em duas normais, ou seja \\(Y_1\\) e \\(Y_2\\) são independentes com marginais Normal(0,2).\n\n\nExercise 3.16 Sejam \\(X_1\\) e \\(X_2\\) variáveis aleatórias independentes com distribuição Normal(0,1).\n\nEncontre a função densidade conjunta de \\(Y_1=aX_1+X_2\\) e \\(Y_2=X_1−X_2\\), onde \\(a\\) é uma constante.\nQuais são os valores de \\(a\\) tais que \\(Y_1\\) é independente de \\(Y_2\\)?\n\n\n\nExample 3.10 Sejam \\(X\\) e \\(Y\\) variáveis aleatórias independentes com distribuição Exponencial(1). Vamos determinar a distribuição conjunta de \\(U=X+Y\\) e \\(V=Y/X\\).\nComo \\(X\\) e \\(Y\\) são independentes, sua densidade conjunta é \\[f(x,y)=f_X(x)f_Y(y)=e^{-x}e^{-y}I_{(0,\\infty)}(x)I_{(0,\\infty)}(y).\\]\nVamos escrever \\((x,y)\\) como função de \\((u,v)\\). Primeiro, \\[u=x+y\\Rightarrow x=u-y.\\] Substituindo \\(x=u-y\\) em \\(v=y/x\\), teremos que\n\\[y=vx=v(u-y)\\Rightarrow y(1+v)=vu\\Rightarrow y=\\frac{uv}{1+v}\\] logo \\[\\begin{cases}\nx=\\frac{u}{1+v}\\\\\ny=\\frac{uv}{1+v}\n\\end{cases}\\] o que implica na matriz jacobiana\n\\[\\mathcal{J}=\\left|\\begin{array}{cc}\\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v}\\\\ \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v} \\end{array}\\right|=\\left|\\begin{array}{cc}\\frac{1}{1+v}&-\\frac{u}{(1+v)^2 } \\\\ \\frac{v}{1+v} & \\frac{u}{(1+v)^2}\\end{array}\\right|\\]\ncujo módulo do determinante é \\[|\\det\\mathcal{J}| = \\left| \\frac{u}{(1+v)^3} - \\left( -\\frac{uv}{(1+v)^3} \\right) \\right| = \\left| \\frac{u(1+v)}{(1+v)^3} \\right| = \\frac{u}{(1+v)^2}\\] Então,\n\\[\\begin{align}f_{U,V}(u,v)&=f_X\\left(\\frac{u}{1+v}\\right)f_Y\\left(\\frac{uv}{1+v}\\right)|\\det\\mathcal{J}|\\\\&=e^{-\\left(\\frac{u}{1+v}\\right)}I_{(0,\\infty)}\\left(\\frac{u}{1+v}\\right)e^{-\\left(\\frac{uv}{1+v}\\right)}I_{(0,\\infty)}\\left(\\frac{uv}{1+v}\\right)\\frac{u}{(1+v)^2}\\\\&=e^{-\\left(\\frac{u}{1+v}+\\frac{uv}{1+v}\\right)}I_{(0,\\infty)}\\left(\\frac{u}{1+v}\\right)I_{(0,\\infty)}\\left(\\frac{uv}{1+v}\\right)\\frac{u}{(1+v)^2}\\\\&=\\frac{u}{(1+v)^2}e^{-u}I_{(0,\\infty)}\\left(\\frac{u}{1+v}\\right)I_{(0,\\infty)}\\left(\\frac{uv}{1+v}\\right)\\end{align}\\] e, notando que as indicadoras são iguais a um quando \\(u,v&gt;0\\), teremos\n\\[\\begin{align}f_{U,V}(u,v)=\\frac{u}{(1+v)^2}e^{-u}I_{(0,\\infty)}\\left(u\\right)I_{(0,\\infty)}\\left(v\\right)\\end{align}\\]\n\n\nExercise 3.17 Sejam \\(X\\) e \\(Y\\) variáveis aleatórias independentes, ambas com distribuição Gama de parâmetros \\(\\a\\) e \\(b\\), especificamente \\(X \\sim \\text{Gama}(a, 1)\\) e \\(Y \\sim \\text{Gama}(b, 1)\\). Considere as seguintes transformações:\\[U = X + Y \\quad \\text{e} \\quad V = \\frac{X}{X+Y}\\]Determine a densidade conjunta \\(f_{U,V}(u, v)\\).Verifique se \\(U\\) e \\(V\\) são independentes.Identifique as distribuições marginais de \\(U\\) e \\(V\\)\nNota A função densidade da distribuição Gamma(\\(a,b\\)) é \\[f(x)=\\frac{b^a}{\\Gamma(a)}x^{a-1}e^{-bx}I_{(0,\\infty)}(x),\\] com \\(a,b&gt;0\\).\n\n\n\n3.5.3 Caso vetorial com \\(g:\\mathbb{R}^n\\rightarrow\\mathbb{R}^n\\), com \\(n&gt;m\\)\nConsidere o agora que \\(g:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m\\), onde \\(n&gt;m\\). Nesse caso, \\(\\textbf{Y}=g(\\textbf{X})\\) ainda é um vetor aleatório, mas não podemos aplicar a integral por substitição \\(g(.)\\) não tem inversa. Neste caso, adicionamos um vetor \\(\\textbf{W}=h(X)\\) de dimensão \\(n−m\\) tal que a função \\[g_*(\\textbf{X})=(g(\\textbf{X}),h(\\textbf{X}))=(\\textbf{y},\\textbf{w})\\] tenha inversa. Então, obtemos a densidade conjunta de \\((\\textbf{Y},\\textbf{W})\\) através do método discutido anteriormente, ou seja\n\\[f_{\\textbf{Y},\\textbf{W}}(\\textbf{y},\\textbf{w})=f_X(g_ *^{−1}(\\textbf{y},\\textbf{w}))|\\det \\mathcal{J}g_ *^{−1}|.\\] Em seguida, podemos encontrar a densidade marginal de \\(\\textbf{Y}\\) integrando a conjunta acima em \\(\\textbf{W}\\):\n\\[f_\\textbf{Y}(\\textbf{y})=\\int f_\\textbf{X}(g_*^{−1}(\\textbf{y},\\textbf{w}))|\\det\\mathcal{J}g_*^{-1}(\\textbf{y},\\textbf{w})|d\\textbf{w}.\\] ::: {#exm-} Soma de duas variáveis aletórias contínuas independentes. Sejam \\(X_1,X_2\\) variáveis aleatórias independentes e considere o problema de encontrar a função densidade de \\(Y=X_1+X_2\\).\nComo \\(y=g(x_1,x_2)=x_1+x_2\\) não tem inversa, vamos utilizar a função \\(g_*(x_1,x_2)=(x_1+x_2,x_2)=(y,w)\\) ( como a escolha de \\(w=x_2\\) é arbitrária, aqui o fizemos de modo conveniente). Teremos que\n\\[\\begin{cases}\ny&= x_1+x_2\\\\ w &= x_1 \\end{cases}\\Rightarrow\\begin{cases}x_1&=y-w\\\\ x_2&=w\\end{cases}\\]\nO determinante da matriz Jacobiana é\n\\[\\det\\mathcal{J}g_*^{-1}=\\left|\\begin{array}{cc}\\frac{\\partial{x_1}}{\\partial{y}} & \\frac{\\partial{x_1}}{\\partial{w}} \\\\ \\frac{\\partial{x_2}}{\\partial{y}} & \\frac{\\partial{x_2}}{\\partial{w}} \\end{array}\\right|=\\det\\left|\\begin{array}{cc}1 & -1 \\\\ 0 & 1\\end{array}\\right|=1\\]\n(aqui, fica claro que \\(w=x_2\\) foi escolhido de modo a simplificar o derteminante), logo\n\\[f_{Y,W}(y,w)=f_{X_1}(y-w)f_{X_2}(w)\\] e a função densidade desejada é\n\\[f_{Y}(y)=\\int_{\\mathbb{R}}f_{X_1}(y-w)f_{X_2}(w)dw.\\]\n:::\n\nExercise 3.18 Sejam \\(X_1\\) e \\(X_2\\) variáveis aleatórias independentes com distribuição Exponencial(1), cuja função densidade é \\[f(x)=e^{-x}I_{(0,\\infty)}(x).\\]\nEncontre a função densidade de \\(Y=X_1+X_2\\).\n\n\nExercise 3.19 Sejam \\(X_1\\) e \\(X_2\\) variáveis aleatórias independentes com distribuição Uniforme(0,1), cuja função densidade é\n\\[f(x)=I_{(0,1)}(x)\\] Encontre a função densidade de \\(Y=X_1+X_2\\).\n\n\n\n3.5.4 Casos especiais\n\nExercise 3.20 Soma de normais independentes.\n\nSejam \\(X_1,X_2\\) variáveis aleatórias independentes com \\(X_i\\sim\\hbox{Normal}(\\mu_i,\\sigma^2_i)\\). Mostre que \\(Y=X_1+X_2\\sim\\hbox{Normal}(\\mu_1+\\mu_2,\\sigma_1^2+\\sigma_2^2)\\).\nSejam \\(X_1,\\ldots,X_n\\) variáveis aleatórias independentes com \\(X_i\\sim\\hbox{Normal}(\\mu_i,\\sigma^2_i)\\). Mostre por indução que \\(Y=X_1+\\cdots+X_n\\sim\\hbox{Normal}(\\sum_{i=1}^n \\mu_i,\\sum_{i=1}^n \\sigma^2_i)\\)\n\n\n\nExercise 3.21 Distribuição Chi Quadrado. A função densidade da distribuição \\(\\chi^2_\\nu\\) é\n\\[f(x) = \\frac{1}{2^{\\nu/2} \\Gamma(\\nu/2)} x^{\\nu/2 - 1} e^{-x/2}I_{(0,\\infty)}(x),\\] onde \\(\\nu&gt;0\\).\n\nSejam \\(X_1,X_2\\) variáveis aleatórias independentes com \\(X_i\\sim\\chi^2_{\\nu_i}\\). Mostre que \\(Y=X_1+X_2\\sim\\chi^2_{\\nu_1+\\nu_2}\\).\nSejam \\(X_1,\\ldots,X_n\\) variáveis aleatórias independentes com \\(X_i\\sim\\chi^2_{\\nu_i}\\). Mostre, por indução, que \\(Y=\\sum_{i=1}^{n}X_i\\sim\\chi^2_{\\sum_{i=1}^n \\nu_i}\\).\nSejam \\(Z_i\\sim\\hbox{Normal}(0,1)\\) variáveis independentes. Sabendo que \\(Z_i^2\\sim\\chi^2_1\\), qual é a distribuição de \\(Z_1^2+\\cdots+Z_n^2?\\)\n\n\n\nExercise 3.22 Distribuição t-Student. Sejam \\(X\\sim\\hbox{Normal}(0,1)\\) e \\(Y\\sim\\chi^2_{\\nu}\\) variáveis independentes. Mostre que a função densidade de\n\\[T=\\frac{X}{\\sqrt{Y/\\nu}}\\] é\n\\[f(t)= \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\nu\\pi} \\, \\Gamma\\left(\\frac{\\nu}{2}\\right)} \\left(1 + \\frac{t^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}}I_\\mathbb{R}(t).\\]\nSugestão. Considere a função \\(g_*(x,y)=(x/\\sqrt{y/\\nu}, y)=(t,w)\\).\n\n\nExercise 3.23 Distribuição F de Snedecor. Sejam \\(U\\) e \\(V\\) variáveis aleatórias independentes tais que \\(U \\sim \\chi^2_{\\nu_1}\\) e \\(V \\sim \\chi^2_{\\nu_2}\\). A distribuição F de Snedecor com \\(\\nu_1\\) e \\(\\nu_2\\) graus de liberdade é definida pela transformação:\\[W = \\frac{U/\\nu_1}{V/\\nu_2}\\]\n\nPara encontrar a densidade de \\(W\\), defina uma variável auxiliar \\(Z = V\\). Encontre a densidade conjunta \\(f_{W,Z}(w, z)\\) utilizando o método do Jacobiano.\nMostre que a densidade marginal de \\(W\\) é dada por:\\[f_W(w) = \\frac{\\Gamma\\left(\\frac{\\nu_1+\\nu_2}{2}\\right)}{\\Gamma\\left(\\frac{\\nu_1}{2}\\right)\\Gamma\\left(\\frac{\\nu_2}{2}\\right)} \\left(\\frac{\\nu_1}{\\nu_2}\\right)^{\\frac{\\nu_1}{2}} \\frac{w^{\\frac{\\nu_1}{2}-1}}{\\left(1 + \\frac{\\nu_1}{\\nu_2}w\\right)^{\\frac{\\nu_1+\\nu_2}{2}}} I_{(0, \\infty)}(w)\\]\nComo a distribuição \\(F\\) se relaciona com a distribuição \\(t\\) de Student? (Dica: Observe a definição de \\(T\\) no exercício anterior e considere o que ocorre com \\(T^2\\)).:::",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distribuição de vetores aleatórios contínuos</span>"
    ]
  },
  {
    "objectID": "vetores_mistos.html",
    "href": "vetores_mistos.html",
    "title": "4  Vetores aleatórios mistos",
    "section": "",
    "text": "É possível que o vetor \\(\\textbf{X}=(\\textbf{X}_d,\\textbf{X}_c)\\) possua um subvetor \\(\\textbf{X}_d\\) composto de variáveis aleatórias discretas e um subvetor \\(\\textbf{X}_c\\) composto de variáveis aleatórias contínuas. Suponha inicialmente que \\(\\textbf{X}_d\\) é independente de \\(\\textbf{X}_c\\). Considere a seguinte probabilidade:\n\\[P(\\textbf{X}_d=\\textbf{x}_d,\\textbf{X}_c\\leq\\textbf{x}_c)=P(\\textbf{X}_d=\\textbf{x}_d)F_{\\textbf{X}_c}(\\textbf{x}_c).\\]\nAssumindo que a função acima é diferenciável em \\(\\textbf{x}_c\\), teremos a existência da função:\n\\[g(\\textbf{x}_d,\\textbf{x}_c)=P(\\textbf{X}_d=\\textbf{x}_d)f_{\\textbf{X}_c}(\\textbf{x}_c).\\]\nA função acima é denominada função de densidade conjunta mista ou função de massa-densidade conjunta. É possível definir essa função mesmo quando \\(\\textbf{X}_c\\) é dependente de \\(\\textbf{X}_d\\). Para tanto, seja \\(B(\\varepsilon)=\\{\\textbf{y}: ||\\textbf{y}-\\textbf{x}_c||&lt;\\varepsilon\\}\\). Então, define-se\n\\[g(\\textbf{x}_d,\\textbf{x}_c)=\\lim_{\\varepsilon\\rightarrow 0} \\frac{P(\\textbf{X}_d=\\textbf{x}_d,\\textbf{X}_c\\in B(\\varepsilon))}{Vol(B(\\varepsilon))}.\\]\nObserve que\n\\[P(\\textbf{X}_d=\\textbf{x}_d,\\textbf{X}_c\\in B(\\varepsilon))=P(\\textbf{X}_d=\\textbf{x}_d|\\textbf{X}_c\\in B(\\varepsilon))P(\\textbf{X}_c\\in B(\\varepsilon)),\\] e, alternativamente: \\[P(\\textbf{X}_d=\\textbf{x}_d,\\textbf{X}_c\\in B(\\varepsilon))=P(\\textbf{X}_c\\in B(\\varepsilon)|\\textbf{X}_d=\\textbf{x}_d)P(\\textbf{X}_d=\\textbf{x}_d).\\]\nPelo Teorema do Valor Médio para integrais, existem \\(\\textbf{a}_1, \\textbf{a}_2 \\in B(\\varepsilon)\\) tais que\n\\[Vol(B(\\varepsilon))f_{\\textbf{X}_c|\\textbf{x}_d}(\\textbf{a}_1|\\textbf{x}_d)P(\\textbf{X}_d=\\textbf{x}_d) = P(\\textbf{X}_d=\\textbf{x}_d|\\textbf{X}_c\\in B(\\varepsilon))Vol(B(\\varepsilon))f_{\\textbf{X}_c}(\\textbf{a}_2).\\]\nDividindo ambos os lados por \\(Vol(B(\\varepsilon))\\) e fazendo \\(\\varepsilon \\rightarrow 0\\), os pontos \\(\\textbf{a}_1\\) e \\(\\textbf{a}_2\\) convergem para \\(\\textbf{x}_c\\), resultando em\n\\[g(\\textbf{x}_d,\\textbf{x}_c)=f_{\\textbf{X}_c|\\textbf{x}_d}(\\textbf{x}_c|\\textbf{x}_d)P(\\textbf{X}_d=\\textbf{x}_d)=P(\\textbf{X}_d=\\textbf{x}_d|\\textbf{X}_c= \\textbf{x}_c)f_{\\textbf{X}_c}(\\textbf{x}_c)\\]\n\nPropriedades\n\nProbabilidades:\n\n\\[P(\\textbf{X}_d\\in A,\\textbf{X}_c\\in B)=\\sum_{\\textbf{x}_d\\in A}\\int_{B}g(\\textbf{x}_d,\\textbf{x}_c)d\\textbf{x}_c.\\] 2. Distribuição marginal de \\(\\textbf{X}_d:\\)\n\\[P(\\textbf{X}_d=\\textbf{x}_d)=\\int g(\\textbf{x}_d,\\textbf{x}_c)d\\textbf{x}_c.\\]\n\nDistribuição marginal de \\(\\textbf{X}_c:\\)\n\n\\[f(\\textbf{x}_c)=\\sum_{\\textbf{x}_d} g(\\textbf{x}_d,\\textbf{x}_c).\\]\n\nCondicional de \\(\\textbf{X}_d|\\textbf{X}_c=\\textbf{x}_c\\):\n\n\\[P(\\textbf{X}_d=\\textbf{x}_d|\\textbf{X}_c=\\textbf{x}_c)=\\frac{g(\\textbf{x}_d,\\textbf{x}_c)}{f(\\textbf{x}_c)}.\\]\n\nCondicional de \\(\\textbf{X}_c|\\textbf{X}_d=\\textbf{x}_d\\):\n\n\\[f(\\textbf{x}_c|\\textbf{x}_d)=\\frac{g(\\textbf{x}_d,\\textbf{x}_c)}{P(\\textbf{X}_d=\\textbf{x}_d)}.\\]\n\n\nExample 4.1 Seja \\(X|y\\sim\\hbox{Bernoulli}(y)\\) e \\(y\\sim\\hbox{Uniforme}(0,1)\\). Então\n\\[g(x,y)=y^x(1-y)^{1-x}I_{\\{0,1\\}}(x)I_{(0,1)}(y).\\] Em particular, a marginal de \\(X\\) é \\[P(X=x)=\\int_0^1 y^{x}(1-y)^{1-x}dy=\\frac{\\Gamma(x+1)\\Gamma(2-x)}{\\Gamma(3)}I_{\\{0,1\\}}(x)\\] e 4\n\\[f(y|x)=\\frac{y^x(1-y)^{1-x}}{\\Gamma(x+1)\\Gamma(2-x)/\\Gamma(3)}=\\frac{\\Gamma(3)}{\\Gamma(x+1)\\Gamma(2-x)}y^{x}(1-y)^{1-x}I_{(0,1)}(y)\\]\n\n\nExercise 4.1 Sejam \\(X|y\\sim\\hbox{Binomial}(n,y)\\) e \\(y\\in\\hbox{Uniforme}(0,1)\\). Encontre a distribuição marginal de \\(X\\) e a condicional \\(Y|X=x\\).\n\n\nExercise 4.2 Sejam \\(X|y\\sim\\hbox{Poisson}(y)\\) e \\(y\\in\\hbox{Exponencial}(1)\\). Encontre a distribuição marginal de \\(X\\) e a condicional \\(Y|X=x\\).\n\n\nExercise 4.3 Sejam \\(X|y\\sim\\hbox{Geométrica}(y)\\) e \\(y\\in\\hbox{Uniforme}(0,1)\\). Encontre a distribuição marginal de \\(X\\) e a condicional \\(Y|X=x\\).\n\nExercício Solução Exercício 2.4Suponha que P(X=x)=12, com x=0,1 . Suponha ainda que Y|X=x∼Normal(x,1) . Mostre que\nP(X=0|Y=y)=(1+exp{y−12})−1\nExercício Solução Exercício 2.5Suponha que X∼Gama(2,1) e Y|X=x∼Poisson(x) . Encontre a densidade de X|Y=y .",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vetores aleatórios mistos</span>"
    ]
  },
  {
    "objectID": "covariancia.html",
    "href": "covariancia.html",
    "title": "5  Covariância, correlação e esperança condicional",
    "section": "",
    "text": "5.1 Covariância\nNeste capítulo, estamos interessados na definição de covariância e correlação para um par \\((X,Y)\\) de variáveis aleatórias, assim como as propriedade de \\(E(Y|X=x)\\). É possível estender esses resultados de modo natural para o par \\((\\textbf{X},\\textbf{Y})\\) de vetores aleatórios, mas isso será feito no curso de Estatística Multivariada I\nSeja \\(\\textbf{x}\\) um vetor de comprimento \\(n\\). A distância de \\(\\textbf{x}\\) até a origem é denominada norma e é calculada por\n\\[||\\textbf{x}||=\\sqrt{\\sum_{i=1}^n x_i^2}.\\]\nAgora, considerando outro vetor \\(\\textbf{y}\\), também de comprimento \\(n\\), definimos o produto interno de \\(\\text{x}\\) e \\(\\textbf{x}\\) por\n\\[\\langle\\textbf{x},\\textbf{y}\\rangle=\\sum_{i=1}^n x_iy_i.\\] Pode-se mostrar que\n\\[\\langle\\textbf{x},\\textbf{y}\\rangle=||\\textbf{x}||||\\textbf{y}||\\cos\\theta,\\] onde \\(\\theta\\) é o ângulo formado entre os vetores \\(\\textbf{x}\\) e \\(\\textbf{y}\\). Portanto, o produto interno possui interpretação direta com a relação entre as direções de \\(\\textbf{x}\\) e \\(\\textbf{y}\\): a) se \\(\\langle\\textbf{x},\\textbf{y}\\rangle\\) é positivo, então os vetores apontam para o mesmo sentido (b) se negativo, eles apontam para o sentidos contrários e (c) se nulo, eles são ortogonais. É imediato que\n\\[||\\textbf{x}||=\\sqrt{\\langle \\textbf{x},\\textbf{x}\\rangle}.\\]\nAgora, sem perda de generalidade, assuma que \\(X\\) e \\(Y\\) são variáveis aleatórias discretas com média zero.\nDefina o produto interno ponderado\n\\[\\langle \\textbf{x},\\textbf{y}\\rangle_P=\\sum_{(x,y)\\in\\mathbb{Z}^2}xyP(X=x,Y=y)=E(XY),\\] onde pode-se notar que cada combinação possível de \\((x,y)\\) é ponderada com sua respectiva probabilidade. Observe que\n\\[||\\textbf{x}||_P=\\sqrt{\\langle \\textbf{x},\\textbf{x}\\rangle_P}=\\sqrt{E(X^2)}=\\sqrt{Var(X)},\\] onde a última igualdade é válida porque \\(E(X)=0\\). Essa quantidade, conhecida como desvio padrão, é interpretada como a norma do vetor \\(\\textbf{x}\\), o que explica porque o desvio padrão é considerado uma medida de distância. Se considerarmos que \\(E(X)=\\mu\\), o centro de massa não é mais zero. Podemos definir o vetor centrado em \\(\\mu\\), \\(\\tilde{\\textbf{x}}=\\textbf{x}-\\mu\\). A norma deste vetor considerando o produto interno ponderado é\n\\[||\\tilde{\\textbf{x}}||_P=\\sqrt{\\langle \\textbf{x}-\\mu,\\textbf{x}-\\mu\\rangle_P}=\\sqrt{E((X-\\mu)^2)}=\\sqrt{Var(X)},\\] que é o desvio padrão tradicional.\nSe consideramos que \\(E(X)=\\mu_X\\) e \\(E(Y)=\\mu_Y\\) são diferentes de zero, o produto interno ponderado dos vetores centrados \\(\\tilde{\\textbf{x}}=\\textbf{x}-\\mu_X\\) e \\(\\tilde{\\textbf{y}}=\\textbf{y}-\\mu_Y\\), teremos\n\\[\\langle \\tilde{\\textbf{x}},\\tilde{\\textbf{y}}\\rangle_P=\\sum_{(x,y)\\in\\mathbb{Z}^2} (x-\\mu_X)(y-\\mu_Y)P(X=x_i,Y=y_j)=E\\left((X-\\mu_X)(Y-\\mu_Y)\\right).\\] Observe que a definição do produto interno ponderado pode ser modificada para um par \\((X,Y)\\) de variáveis aleatórias contínuas:\n\\[\\langle \\textbf{x},\\textbf{y}\\rangle_P=\\int_{\\mathbb{R}^2}xyf(x,y)dxdy=E(XY),\\] o que gera as mesmas interpretações dadas para o caso discreto. Podemos então definir formalmente a covariância entre \\(X\\) e \\(Y\\).\nPode-se mostrar que\n\\[Cov(X,Y)=E(XY)-E(X)E(Y),\\] o que é, em geral, mais simples de se obter. Na prática, a covariância dá a relação entre as variáveis \\(X\\) e \\(Y\\) centradas em seus respectivos centros de massa: (a) Se \\(Cov(X,Y)&lt;0\\), então espera-se que o aumento(decrescimento) de \\(X\\) implique em um decrescimento(aumento) de \\(Y\\), (b) se \\(Cov(X,Y)&gt;0\\), então espera-se que o aumento(decrescimento) de \\(X\\) implique em um aumento(decrescimento) de \\(Y\\) e (c) \\(Cov(X,Y)=0\\) implica na ausência de relação linear entre \\(X\\) e \\(Y\\).\n::: {#exr-} Suponha que \\(X\\) e \\(Y\\) assumem valores em \\(\\{−1,0,1\\}\\) com a seguinte função de probabilidade conjunta:\n\\[P(X=x,Y=y)=\\frac{1}{5}\\] se, \\((x,y)\\in\\{(1,1),(−1,1),(1,−1),(−1,−1),(0,0)\\}\\) e \\(P(X=x,Y=y)=0\\) em caso contrário. Mostre que \\(X\\) e \\(Y\\) não são independentes e que \\(Cov(X,Y)=0\\). :::",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Covariância, correlação e esperança condicional</span>"
    ]
  },
  {
    "objectID": "covariancia.html#covariância",
    "href": "covariancia.html#covariância",
    "title": "5  Covariância, correlação e esperança condicional",
    "section": "",
    "text": "Definition 5.1 Sejam \\(X\\) e \\(Y\\) duas variáveis aleatórias quaisquer. Então, a covariância entre \\(X\\) e \\(Y\\) é dada por\n\\[Cov(X,Y)=E((X−E(X))(Y−E(Y))),\\]\n\n\n\n\nExample 5.1 A tabela abaixo mostra a função de probabilidade conjunta \\[\\begin{array}{|c|ccc|c|}\n\\hline\nX \\setminus Y & 1 & 2 & 3 & P(X=x) \\\\ \\hline\n0 & 0,1 & 0,2 & 0,3 & \\mathbf{0,6} \\\\\n1 & 0,2 & 0,1 & 0,1 & \\mathbf{0,4} \\\\ \\hline\nP(Y=y) & \\mathbf{0,3} & \\mathbf{0,3} & \\mathbf{0,4} & \\mathbf{1,0} \\\\ \\hline\n\\end{array}\\]\nVamos encontrar a covariância de \\((X,Y)\\):\n\nEsperanças marginais:\n\n\\[\\begin{align}E(X)&=0\\cdot 0,6+1\\cdot0,4=0,4\\\\\nE(Y)&=1\\cdot0,3+2\\cdot0,3+3\\cdot0,4=2,1\\end{align}\\]\n\nEsperança do produto\n\n\\[\\begin{align}E(XY)&=\\sum_{x=0}^1\\sum_{y=1}^2P(X=x,Y=y)\\\\&=\\sum_{y=1}^2 0\\cdot yP(X=0,Y=y)+\\sum_{y=1}^2 1\\cdot yP(X=0,Y=y)\\\\&=1\\cdot0,2+2\\cdot0,1+3\\cdot0,1=0,7\\end{align}\\] * Resultado da Covariância\n\\[Cov(X, Y) = 0,7 - 0,4\\cdot 2,1=0,7-0,84 = -0,14\\]\n\nInterpretação: A covariância negativa indica que as variáveis tendem a se mover em direções opostas.\n\n\n\nExercise 5.1 A tabela abaixo apresenta a função de probabilidade conjunta para \\((X,Y)\\).\\[\\begin{array}{|c|ccc|c|}\n\\hline\nX \\setminus Y & 1 & 2 & 3 & P(X=x) \\\\ \\hline\n0 & 0,05 & 0,15 & 0,30 & \\mathbf{0,50} \\\\\n1 & 0,25 & 0,15 & 0,10 & \\mathbf{0,50} \\\\ \\hline\nP(Y=y) & \\mathbf{0,30} & \\mathbf{0,30} & \\mathbf{0,40} & \\mathbf{1,00} \\\\ \\hline\n\\end{array}\\]\nDetermine a covariância \\(Cov(X, Y)\\) e interprete o resultado.\n\n\nExercise 5.2 Seja\n\\[f(x,y)=2xy+\\frac{1}{2},\\] para \\((x,y)\\in(0,1)\\). Encontre a covariância de \\((X,Y)\\) e interprete o resultado.\n\n\nPropriedades\n\n\\(Cov(X,X)=Var(X)\\)\nPara \\(a\\)e \\(b\\) constantes e reais, \\(Cov(aX,bY)=abCov(X,Y)\\)\nPara \\(a\\) constante real, \\(Cov(X,a)=0\\)\n\\(Cov(X+Y,Z)=Cov(X,Z)+Cov(Y,Z)\\) e \\(Cov(X,Z+Y)=Cov(X,Z)+Cov(X,Y)\\)\n\n\n\nProposição Se \\(X\\) e \\(Y\\) são independentes, então \\(Cov(X,Y)=0\\).\nImportante: \\(Cov(X,Y)=0\\) não implica em \\(X\\) e \\(Y\\) serem independentes!\n\n\nExample 5.2 Suponha que\n\\[P(X=x,Y=y)=\\frac{1}{4}\\], com \\((x,y)\\in\\{−1,1\\}^2\\). Então\n\\[P(X=x)P(Y=y)=P(X=x,Y=−1)+P(X=x,Y=1)=\\frac{1}{2}=P(X=−1,Y=y)+P(X=1,Y=y)=\\frac{1}{2}.\\] Como\n\\[\\frac{1}{4}=P(X=x,Y=y)=\\frac{1}{2}\\frac{1}{2}=P(X=x)P(Y=y),\\] temos que \\(X\\) e \\(Y\\) são independentes e, portanto \\(Cov(X,Y)=0\\).\n\n\n\n\nExercise 5.3 Considere a densidade conjunta\n\\[f(x,y)=\\frac{1}{y}e^{−(y+xy)}\\], para \\(x,y&gt;0\\). Encontre Cov(X,Y)\n\n\nExercise 5.4 Verifique\n\n\\(Cov(X,Y)=Cov(Y,X)\\)\n\\(Cov(X,X)=Var(X)\\)\n\\(Cov(aX,Y)=Cov(X,aY)=aCov(X,Y)\\), \\(a\\) constante.\n\nConclua que \\(Cov(aX,bY)=abCov(X,Y)\\), com \\(a,b\\) constantes.\n\n\nExercise 5.5 Sejam \\(X_1,\\ldots,X_n\\) e \\(Y_1,\\ldots,Y_m\\) variáveis aleatórias. Mostre que\n\\[Cov(\\sum_{i=1}^nX_i,Y_j)=\\sum_{i=1}^nCov(X_i,Y_j), \\forall\\;\\; j=1,\\ldots,m\\]\n\\[Cov(X_i,\\sum_{j=1}^m Y_j)=\\sum_{j=1}^mCov(X_i,Y_j), \\forall\\;\\; i=1,\\ldots,n\\]\nConclua que\n\\[Cov(\\sum_{i=1}^n X_i,\\sum_{j=1}^m Y_j)=\\sum_{i=1}^n\\sum_{j=1}^mCov(X_i,Y_j).\\]\n\n\nExercise 5.6 Sejam \\(X,Y\\) duas variáveis aleatórias. Mostre que\n\\[Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y).\\]\nAgora, sejam \\(X_1,\\ldots,X_n\\) variáveis aleatórias. Mostre que\n\\[Var(\\sum_{i=1}^n X_i)=\\sum_{i=1}^nVar(X_i)+\\sum_{i=1}^n\\sum_{j=1}^nCov(X_i,X_j).\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Covariância, correlação e esperança condicional</span>"
    ]
  },
  {
    "objectID": "covariancia.html#correlação",
    "href": "covariancia.html#correlação",
    "title": "5  Covariância, correlação e esperança condicional",
    "section": "5.2 Correlação",
    "text": "5.2 Correlação\nRecorde que, para o produto interno entre vetores, é verdadeiro que\n\\[\\langle \\textbf{x},\\textbf{y}\\rangle=||\\textbf{x}||||\\textbf{y}||\\cos\\theta,\\] onde \\(\\theta\\) é o ângulo formado entre os dois vetores. Logo, tem-se que\n\\[\\cos\\theta=\\frac{\\langle \\textbf{x},\\textbf{y}\\rangle}{||\\textbf{x}||||\\textbf{y}||},\\]\nEm particular, se \\(|\\cos\\theta|=1\\), então \\(\\textbf{x}\\) e \\(\\textbf{y}\\) são paralelos, o que implica que exsite \\(b\\) tal que \\(\\textbf{y}=v\\textbf{x}\\) e \\(b\\) possui o mesmo sinal do cosseno.\nPodemos estender essa noção para o par de variáveis aleatórias \\((X,Y)\\), definindo o coeficiente de correlação de Pearson:\n\\[\\rho_{XY}=\\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}},\\] que é o cosseno do ângulo entre as variáveis centralizadas \\(X-E(X)\\) e \\(Y-E(Y)\\). Esse coeficiente mede o quanto \\(Y\\) e \\(X\\) podem ser escritas como função linear uma da outra. De fato, se \\(|\\rho_{XY}|=1\\), então existem \\(a\\) e \\(b\\) tais que\n\\[P(Y=a+bX)=1,\\] onde o sinal de \\(b\\) é o mesmo de \\(\\rho_{XY}\\).\n\nExample 5.3 Sejam \\(X\\) e $Y variáveis aleatórias com densidade conjunta\n\\[f(x,y)=\\frac{1}{2\\pi}\\exp\\left\\{−\\frac{1}{2}(x^2+2y^2−2xy)\\right\\},\\] com \\(x,y\\in\\mathbb{R}\\). Vamos encontrar a correlação entre \\(X\\) e \\(Y\\).\n…",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Covariância, correlação e esperança condicional</span>"
    ]
  },
  {
    "objectID": "covariancia.html#esperança-condicional",
    "href": "covariancia.html#esperança-condicional",
    "title": "5  Covariância, correlação e esperança condicional",
    "section": "5.3 Esperança condicional",
    "text": "5.3 Esperança condicional\nA esperança \\(X\\) condicionada à \\(Y=y\\) é denotada por \\(E(X|Y=y)\\) e se refere à esperança obtida a partir da distribuição de \\((X|Y=y)\\). Se \\(X\\) é uma variável aleatória contínua, a esperança condicional é dada por\n\\[E(X|Y=y)=\\int_{\\mathbb{R}}x f(x|y)dx, \\] e se \\(X\\) for discreta, \\[E(X|Y=y)=\\sum_{x\\in\\mathbb{Z}} xP(X=x|Y=y).\\]\nDe modo análogo, para qualquer função real \\(g(.)\\) podemos definir \\(E(g(X)|Y=y)\\).\nA variância condicional de \\(X\\) dado \\(Y=y\\) é definida por:\n\\[Var(X|Y=y)=E((X−E(X|Y=y))^2)=E(X^2|Y=y)−E(X|Y=y)^2.\\]\nExercício Solução Seja (X,Y) um vetor aleatório com a seguinte função de densidade conjunta f(x,y)=11−x, para 0&lt;x&lt;y&lt;1 . Encontre E(Y|x) e Var(Y|x) .\nProposição Demonstração Sejam X e Y variáveis aleatórias com esperança finita. Então,\nE(X)=E(E(X|Y)). Além disso, se a variância for finita, então, Var(X)=E(Var(X|Y))+Var(E(X|Y)).\nExercício Solução Suponha que X|y∼Poisson(y) e Y∼Gama(a,b) . Encontre a média e a variância de X .\nExercícios de fixação\nSuponha que o vetor aleatório (X,Y) tem a seguinte densidade conjunta f(x,y)=ya+12−12a2Γ(a/2)2π−−√exp{−y2(1+x2)}, onde y&gt;0 , x∈R e a&gt;1 .\nMostre Y∼Gama(a/2,1/2)\nMostre que X|y∼Normal(0,1/y)\nEncontre E(X) e Var(X) .\nConsidere a seguinte definição: Var(X|Y)=E[(X−E(X|Y))2|Y]. Prove que Var(X)=E(Var(X|Y))+Var(E(X|Y))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Covariância, correlação e esperança condicional</span>"
    ]
  }
]